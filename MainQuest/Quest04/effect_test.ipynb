{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "import sys # 오류 처리 시 필요\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 전역 설정 및 모델, MediaPipe, 웹캠 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_gif = Image.open('./img/sunflower.gif')\n",
    "\n",
    "# 모든 프레임 추출\n",
    "flower_frames = []\n",
    "try:\n",
    "    while True:\n",
    "        frame = flower_gif.convert('RGBA')\n",
    "        frame_np = np.array(frame)\n",
    "        frame_cv = cv2.cvtColor(frame_np, cv2.COLOR_RGBA2BGRA)\n",
    "        flower_frames.append(np.array(frame_cv))\n",
    "        flower_gif.seek(flower_gif.tell() + 1)\n",
    "except EOFError:\n",
    "    pass\n",
    "\n",
    "num_flower_frames = len(flower_frames)\n",
    "frame_idx = 0\n",
    "\n",
    "# GIF 합성 함수 정의\n",
    "def get_flower(image, result, frame_idx):\n",
    "    wrist1 = result.multi_hand_landmarks[0].landmark[0]\n",
    "    wrist2 = result.multi_hand_landmarks[1].landmark[0]\n",
    "\n",
    "    wrist1_x = int(wrist1.x * image.shape[1])\n",
    "    wrist2_x = int(wrist2.x * image.shape[1])\n",
    "    wrist1_y = int(wrist1.y * image.shape[0])\n",
    "\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[9]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[9]\n",
    "\n",
    "    hand1_x = int(hand1.x * image.shape[1])\n",
    "    hand2_x = int(hand2.x * image.shape[1])\n",
    "    hand1_y = int(hand1.y * image.shape[0])\n",
    "    hand2_y = int(hand2.y * image.shape[0])\n",
    "\n",
    "    w1 = max(hand1_x, hand2_x) - min(hand1_x, hand2_x)\n",
    "    w2 = max(hand1_y, hand2_y) - min(hand1_y, hand2_y)\n",
    "\n",
    "    w = max(w1, w2)\n",
    "    x = min(wrist1_x, wrist2_x)\n",
    "    y = wrist1_y\n",
    "\n",
    "    if w == 0:\n",
    "        return image\n",
    "\n",
    "    overlay_img = flower_frames[frame_idx % num_flower_frames]\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w * overlay_img.shape[0] / overlay_img.shape[1])))\n",
    "\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 이미지 경계 넘어가지 않도록 crop 처리\n",
    "    y1 = max(0, y - (h//2))\n",
    "    y2 = min(y1 + h, image.shape[0])\n",
    "    x1 = max(0, x - (w//2))\n",
    "    x2 = min(image.shape[1], x1 + w)\n",
    "\n",
    "    overlay_crop = overlay_img[0:(y2 - y1), 0:(x2 - x1), :] # overlay_img[(h - (y2 - y1)):, (x1 - x):(x2 - x), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1:x2, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1:x2, c] * (1 - mask_crop))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crown = cv2.imread('./img/crown.png', cv2.IMREAD_UNCHANGED)\n",
    "crown_ratio = crown.shape[0] / crown.shape[1]\n",
    "\n",
    "def get_crown(image, result):   # 프레임 이미지, 적용 좌표, 높이 너비, 효과 이미지\n",
    "    # if result.multi_hand_landmarks is None or len(result.multi_hand_landmarks) < 2:\n",
    "    #     return image\n",
    "    hand1 = result.multi_hand_landmarks[0].landmark[0]\n",
    "    hand2 = result.multi_hand_landmarks[1].landmark[0]\n",
    "\n",
    "    hand1_x = int(hand1.x * image.shape[1])\n",
    "    hand2_x = int(hand2.x * image.shape[1])\n",
    "    hand1_y = int(hand1.y * image.shape[0])\n",
    "\n",
    "    w = max(hand1_x, hand2_x) - min(hand1_x, hand2_x)\n",
    "    x = min(hand1_x, hand2_x)\n",
    "    y = hand1_y\n",
    "\n",
    "    if w == 0:\n",
    "        return image # 두 손이 같은 위치면 crown 그리지 않음\n",
    "    \n",
    "    overlay_img = crown.copy()\n",
    "    overlay_img = cv2.resize(overlay_img, (w, int(w*crown_ratio)))\n",
    "\n",
    "    # alpha = overlay_img[:, :, 3]\n",
    "    # mask = alpha / 255\n",
    "\n",
    "    h = overlay_img.shape[0]\n",
    "\n",
    "    # 이미지 경계 넘는 부분 잘라내기\n",
    "    y1 = max(0, y - h)\n",
    "    y2 = y\n",
    "    x1_crop = max(0, x)\n",
    "    x2_crop = min(image.shape[1], x + w)\n",
    "\n",
    "    overlay_crop = overlay_img[(h - (y2 - y1)):, (x1_crop - x):(x2_crop - x), :]\n",
    "    mask_crop = overlay_crop[:, :, 3] / 255\n",
    "\n",
    "    for c in range(3):\n",
    "        image[y1:y2, x1_crop:x2_crop, c] = \\\n",
    "            (overlay_crop[:, :, c] * mask_crop) + (image[y1:y2, x1_crop:x2_crop, c] * (1 - mask_crop))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from './models/model_v1.h5'\n",
      "MediaPipe Hands model initialized for two hands.\n",
      "웹캠이 성공적으로 초기화되었습니다.\n",
      "웹캠 해상도: 640x480\n",
      "Defined actions for prediction: ['flower', 'crown', 'heart_beat', 'firework', 'bear', 'cat', 'son_celebration', 'heart_ont_the_cheek', 'gun', 'pipe', 'tiger', 'landmarks']\n",
      "Sequence length for model input: 30\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Global Setup and Initialization\n",
    "\n",
    "# --- 2.1 학습된 모델 로드 ---\n",
    "model_path = './models/model_v1.h5' # 실제 모델 파일 경로\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Model loaded successfully from '{model_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from '{model_path}': {e}\")\n",
    "    print(\"모델 파일이 없거나 경로가 잘못되었습니다. 경로를 확인해주세요.\")\n",
    "    model = None # 모델 로드 실패 시 None으로 설정하여 이후 예측 단계에서 에러 방지\n",
    "\n",
    "\n",
    "# --- 2.2 MediaPipe Hands 모델 초기화 (두 손 감지 설정) ---\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2, # 두 손 감지 설정\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "print(\"MediaPipe Hands model initialized for two hands.\")\n",
    "\n",
    "\n",
    "# --- 2.3 웹캠 초기화 ---\n",
    "cap = cv2.VideoCapture(0) # 기본 웹캠 (대부분 0번)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"\\n--- 오류: 웹캠을 열 수 없습니다! ---\")\n",
    "    print(\"   웹캠이 연결되어 있는지, 드라이버가 올바른지, 다른 프로그램에서 사용 중이 아닌지 확인해주세요.\")\n",
    "    print(\"   이 오류가 발생하면 이후 실시간 추론 셀은 작동하지 않습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"웹캠이 성공적으로 초기화되었습니다.\")\n",
    "    # 웹캠 해상도 설정\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    print(f\"웹캠 해상도: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "\n",
    "\n",
    "# --- 2.4 제스처 이름 및 시퀀스 길이 설정 ---\n",
    "# 학습 시 사용했던 actions 리스트와 seq_length 값이 정확히 일치해야 합니다!\n",
    "actions = [\n",
    "    'flower', 'crown', 'heart_beat',\n",
    "    'firework', 'bear', 'cat',\n",
    "    'son_celebration', 'heart_ont_the_cheek', 'gun',\n",
    "    'pipe', 'tiger', 'landmarks'\n",
    "]\n",
    "seq_length = 30 # 학습 시 사용한 시퀀스 길이와 동일해야 합니다.\n",
    "\n",
    "print(f\"Defined actions for prediction: {actions}\")\n",
    "print(f\"Sequence length for model input: {seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 실시간 제스처 추론 메인 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 실시간 제스처 추론 시작 ---\n",
      "   웹캠 창이 열리면 화면을 보면서 제스처를 수행하세요.\n",
      "   'q' 키를 누르면 언제든지 종료할 수 있습니다.\n",
      "카메라 스트림 시작...\n",
      "\n",
      "--- 추론 중 치명적인 오류 발생 ---\n",
      "   오류 내용: list index out of range\n",
      "\n",
      "--- 리소스 정리 중... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\samba\\AppData\\Local\\Temp\\ipykernel_32628\\322625665.py\", line 117, in <module>\n",
      "    img = get_crown(img, result)\n",
      "  File \"C:\\Users\\samba\\AppData\\Local\\Temp\\ipykernel_32628\\3225131281.py\", line 6, in get_crown\n",
      "    hand2 = result.multi_hand_landmarks[1].landmark[0]\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   웹캠 리소스가 해제되었습니다.\n",
      "   모든 OpenCV 창이 닫혔습니다.\n",
      "   MediaPipe Hands 모델 리소스가 해제되었습니다.\n",
      "\n",
      "추론 스크립트 실행 완료.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Real-time Gesture Prediction Main Loop\n",
    "\n",
    "# --- 3.1 시퀀스 데이터 저장을 위한 버퍼 초기화 ---\n",
    "# LSTM 모델은 과거 'seq_length'만큼의 프레임 데이터를 필요로 합니다.\n",
    "# collections.deque는 고정된 최대 길이를 가지는 큐입니다.\n",
    "seq_data_buffer = deque(maxlen=seq_length)\n",
    "\n",
    "# --- 3.2 화면 표시 설정 ---\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_thickness = 2\n",
    "text_color_default = (255, 255, 255) # White\n",
    "\n",
    "print(\"\\n--- 실시간 제스처 추론 시작 ---\")\n",
    "print(\"   웹캠 창이 열리면 화면을 보면서 제스처를 수행하세요.\")\n",
    "print(\"   'q' 키를 누르면 언제든지 종료할 수 있습니다.\")\n",
    "\n",
    "try:\n",
    "    if not cap.isOpened() or model is None:\n",
    "        print(\"웹캠 또는 모델이 준비되지 않아 추론을 시작할 수 없습니다. 이전 셀의 오류를 확인하세요.\")\n",
    "    else:\n",
    "        print(\"카메라 스트림 시작...\")\n",
    "        # 무한 루프를 사용하여 웹캠 프레임을 계속 읽고 처리합니다.\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            # 프레임 읽기 실패 시 루프 종료\n",
    "            if not ret:\n",
    "                print(\"프레임을 읽을 수 없습니다. 카메라 연결 또는 웹캠 상태를 확인하고 루프를 종료합니다.\")\n",
    "                break\n",
    "\n",
    "            img = cv2.flip(img, 1) # 좌우 반전 (거울 모드)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # MediaPipe 처리를 위해 BGR -> RGB 변환\n",
    "            result = hands.process(img_rgb) # MediaPipe를 이용한 손 랜드마크 감지\n",
    "            img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR) # OpenCV 표시를 위해 RGB -> BGR 변환\n",
    "\n",
    "            # --- 3.3 현재 프레임의 특징 데이터 추출 및 패딩 ---\n",
    "            # 학습 시 사용했던 두 손 데이터를 구성하는 로직과 동일해야 합니다.\n",
    "            current_frame_features = []\n",
    "            num_detected_hands = 0\n",
    "            display_message = \"\"\n",
    "            current_text_color = text_color_default\n",
    "\n",
    "            if result.multi_hand_landmarks:\n",
    "                num_detected_hands = len(result.multi_hand_landmarks)\n",
    "                # 두 손이 모두 감지되었을 때만 실제 특징 데이터 처리\n",
    "                if num_detected_hands == 2:\n",
    "                    # 손의 x좌표를 기준으로 정렬하여 항상 일관된 순서(예: 왼손, 오른손)로 데이터를 구성\n",
    "                    handedness_sorted = [(lm.landmark[0].x, i) for i, lm in enumerate(result.multi_hand_landmarks)]\n",
    "                    handedness_sorted.sort()\n",
    "\n",
    "                    for _, hand_idx in handedness_sorted:\n",
    "                        res = result.multi_hand_landmarks[hand_idx]\n",
    "\n",
    "                        # 랜드마크 추출 (21개 관절 * 4차원 = 84개 특징)\n",
    "                        joint = np.zeros((21, 4))\n",
    "                        for j, lm in enumerate(res.landmark):\n",
    "                            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                        current_frame_features.extend(joint.flatten().tolist())\n",
    "\n",
    "                        # 관절 각도 계산 (15개 특징)\n",
    "                        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]\n",
    "                        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]\n",
    "                        v = v2 - v1\n",
    "                        norm_v = np.linalg.norm(v, axis=1)\n",
    "                        v = v / (norm_v[:, np.newaxis] + 1e-8) # 0으로 나누는 오류 방지\n",
    "                        dot_product = np.einsum('nt,nt->n',\n",
    "                                                 v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],\n",
    "                                                 v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])\n",
    "                        dot_product = np.clip(dot_product, -1.0, 1.0) # arccos 도메인 유지\n",
    "                        angle = np.degrees(np.arccos(dot_product))\n",
    "                        current_frame_features.extend(angle.tolist())\n",
    "\n",
    "                        mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS) # 화면에 랜드마크 그리기\n",
    "\n",
    "                    # 두 손의 특징 데이터가 모두 추출되었는지 최종 확인 (총 198개)\n",
    "                    expected_features_len = (21 * 4 + 15) * 2\n",
    "                    if len(current_frame_features) != expected_features_len:\n",
    "                        print(f\"특징 개수 불일치! (예상: {expected_features_len}, 실제: {len(current_frame_features)})\")\n",
    "                        # 불일치 시 0으로 패딩하여 모델 입력 shape 유지\n",
    "                        current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                        display_message = \"Data Error - Padding\"\n",
    "                        current_text_color = (0, 0, 255) # Red\n",
    "                else:\n",
    "                    # 손이 감지되었지만 2개가 아닌 경우 (1개 또는 3개 이상)\n",
    "                    expected_features_len = (21 * 4 + 15) * 2 # 항상 198개\n",
    "                    current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                    display_message = f\"Detected {num_detected_hands} hand(s). Need 2.\"\n",
    "                    current_text_color = (0, 165, 255) # Orange\n",
    "            else:\n",
    "                # 손이 전혀 감지되지 않은 경우\n",
    "                expected_features_len = (21 * 4 + 15) * 2 # 항상 198개\n",
    "                current_frame_features = np.zeros(expected_features_len, dtype=np.float32).tolist()\n",
    "                display_message = \"No hands detected.\"\n",
    "                current_text_color = (0, 0, 255) # Red\n",
    "\n",
    "            # 추출된 (또는 패딩된) 특징 데이터를 시퀀스 버퍼에 추가\n",
    "            seq_data_buffer.append(np.array(current_frame_features, dtype=np.float32))\n",
    "\n",
    "\n",
    "            # --- 3.4 시퀀스 버퍼가 충분히 채워지면 예측 수행 ---\n",
    "            if len(seq_data_buffer) == seq_length:\n",
    "                # 모델 입력 형태에 맞게 차원 확장 (배치 차원 추가)\n",
    "                input_sequence = np.expand_dims(np.array(seq_data_buffer), axis=0) # (1, seq_length, num_features)\n",
    "\n",
    "                if model is not None: # 모델이 성공적으로 로드되었는지 확인\n",
    "                    preds = model.predict(input_sequence, verbose=0)[0] # verbose=0으로 예측 진행바 숨기기\n",
    "                    action_idx = np.argmax(preds)\n",
    "                    confidence = preds[action_idx]\n",
    "\n",
    "                    # 신뢰도가 높은 예측만 표시 ( threshold 조절 가능)\n",
    "                    if confidence > 0.7: # 70% 이상의 신뢰도일 때만 제스처 이름 표시\n",
    "                        predicted_action = actions[action_idx]\n",
    "                        display_message = f\"Action: {predicted_action} ({confidence:.2f})\"\n",
    "                        current_text_color = (0, 255, 0) # Green\n",
    "                        if predicted_action == 'crown':\n",
    "                            img = get_crown(img, result)\n",
    "                        elif predicted_action == 'flower':\n",
    "                            img = get_flower(img, result, frame_idx)\n",
    "                            frame_idx += 1\n",
    "                    else:\n",
    "                        display_message = \"Action: Thinking...\" # 신뢰도가 낮으면 \"생각 중\"으로 표시\n",
    "                        current_text_color = (0, 165, 255) # Orange\n",
    "                else:\n",
    "                    display_message = \"Model not loaded.\" # 모델이 로드되지 않았을 경우\n",
    "                    current_text_color = (0, 0, 255) # Red\n",
    "            else:\n",
    "                # 버퍼가 채워지는 동안 메시지 표시\n",
    "                display_message = f\"Buffering... ({len(seq_data_buffer)}/{seq_length})\"\n",
    "                current_text_color = (255, 255, 0) # Yellow\n",
    "\n",
    "\n",
    "            # --- 3.5 웹캠 화면에 정보 표시 및 업데이트 ---\n",
    "            cv2.putText(img, display_message, (10, 60), font, font_scale, current_text_color, font_thickness)\n",
    "            cv2.imshow('Gesture Recognition (Two Hands)', img)\n",
    "\n",
    "            # --- 3.6 종료 조건 확인 ('q' 키) ---\n",
    "            key = cv2.waitKey(1) & 0xFF # 1ms 대기 및 키 입력 확인\n",
    "            if key == ord('q'):\n",
    "                print(\"사용자 요청 ('q' 키)으로 추론을 중단합니다.\")\n",
    "                break\n",
    "            # 참고: 창 닫기 버튼으로 종료하는 기능은 Jupyter 환경에서 불안정할 수 있습니다.\n",
    "            # `if cv2.getWindowProperty('Gesture Recognition (Two Hands)', cv2.WND_PROP_VISIBLE) < 1:` 와 같은 코드는\n",
    "            # 특정 환경에서 오류를 유발할 수 있어 일반적으로 'q' 키를 권장합니다.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- 추론 중 치명적인 오류 발생 ---\")\n",
    "    print(f\"   오류 내용: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # 상세한 에러 스택 트레이스 출력 (디버깅에 유용)\n",
    "\n",
    "finally:\n",
    "    # --- 3.7 리소스 정리 (예외 발생 여부와 상관없이 항상 실행) ---\n",
    "    print(\"\\n--- 리소스 정리 중... ---\")\n",
    "    if 'cap' in locals() and cap.isOpened():\n",
    "        cap.release()\n",
    "        print(\"   웹캠 리소스가 해제되었습니다.\")\n",
    "    else:\n",
    "        print(\"   웹캠이 이미 닫혀 있거나 초기화되지 않았습니다.\")\n",
    "\n",
    "    cv2.destroyAllWindows() # 모든 OpenCV 창 닫기\n",
    "    print(\"   모든 OpenCV 창이 닫혔습니다.\")\n",
    "\n",
    "    if 'hands' in locals():\n",
    "        hands.close() # MediaPipe Hands 모델 리소스 해제\n",
    "        print(\"   MediaPipe Hands 모델 리소스가 해제되었습니다.\")\n",
    "\n",
    "print(\"\\n추론 스크립트 실행 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
