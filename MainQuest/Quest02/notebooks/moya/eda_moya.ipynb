{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84beb7e9",
   "metadata": {},
   "source": [
    "## ë°ì´íƒ€ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a885d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5ee4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1951</td>\n",
       "      <td>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>íŒ€ì¥ë‹˜ ì´ê±° ì–¸ì œê¹Œì§€ ë§ˆë¬´ë¦¬ í•˜ë©´ ë ê¹Œìš”?\\në¬´ë¦¬í•˜ì§€ ë§ê³  ë„‰ë„‰í•˜ê²Œ ì£¼ë§ê¹Œì§€ ë‹¤ ì‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4756</td>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ë‚´ì¼ ë‚ ì”¨ ì–´ë–»ëŒ€?\\në¹„ ì˜¨ë‹¤ë˜ë°. ìš°ì‚° ì±™ê²¨ê°€ì•¼ í•  ê²ƒ ê°™ì•„.\\nì—ì´, ì•¼ì™¸ í™œë™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1234</td>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì•¼ ìŸ¤ ì¢€ ë´.\\n ê¼´ì— ìœ í–‰í•˜ëŠ” ì˜· ì…ì—ˆë„¤ \\n í˜¸ë°•ì— ì¤„ ê¸‹ëŠ”ë‹¤ê³  ìˆ˜ë°•ë˜ë‚˜ \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4767</td>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ì˜¤ëŠ˜ ìˆ˜ì—… ë‚´ìš© ì´í•´í–ˆì–´?\\nì†”ì§íˆ ì¢€ ì–´ë ¤ì› ì–´. ë„ˆëŠ”?\\në‚˜ë„ ëª‡ ë¶€ë¶„ì´ í—·ê°ˆë¦¬ë”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1511</td>\n",
       "      <td>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ìë„¤ ë‚´ì¼ ì•„ì¹¨ì— ìš°ë¦¬ì§‘ ë“¤ë ¤ì„œ ì¶œê·¼í•˜ê²Œ\\në„¤?\\në‚´ê°€ ì°¨ê°€ ê³ ì¥ë‚¬ì–´\\nì•„. ê·¸ëŸ°ë°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx        class                                       conversation\n",
       "0  1951  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  íŒ€ì¥ë‹˜ ì´ê±° ì–¸ì œê¹Œì§€ ë§ˆë¬´ë¦¬ í•˜ë©´ ë ê¹Œìš”?\\në¬´ë¦¬í•˜ì§€ ë§ê³  ë„‰ë„‰í•˜ê²Œ ì£¼ë§ê¹Œì§€ ë‹¤ ì‘...\n",
       "1  4756        ì¼ë°˜ ëŒ€í™”  ë‚´ì¼ ë‚ ì”¨ ì–´ë–»ëŒ€?\\në¹„ ì˜¨ë‹¤ë˜ë°. ìš°ì‚° ì±™ê²¨ê°€ì•¼ í•  ê²ƒ ê°™ì•„.\\nì—ì´, ì•¼ì™¸ í™œë™...\n",
       "2  1234    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì•¼ ìŸ¤ ì¢€ ë´.\\n ê¼´ì— ìœ í–‰í•˜ëŠ” ì˜· ì…ì—ˆë„¤ \\n í˜¸ë°•ì— ì¤„ ê¸‹ëŠ”ë‹¤ê³  ìˆ˜ë°•ë˜ë‚˜ \\n...\n",
       "3  4767        ì¼ë°˜ ëŒ€í™”  ì˜¤ëŠ˜ ìˆ˜ì—… ë‚´ìš© ì´í•´í–ˆì–´?\\nì†”ì§íˆ ì¢€ ì–´ë ¤ì› ì–´. ë„ˆëŠ”?\\në‚˜ë„ ëª‡ ë¶€ë¶„ì´ í—·ê°ˆë¦¬ë”...\n",
       "4  1511  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ìë„¤ ë‚´ì¼ ì•„ì¹¨ì— ìš°ë¦¬ì§‘ ë“¤ë ¤ì„œ ì¶œê·¼í•˜ê²Œ\\në„¤?\\në‚´ê°€ ì°¨ê°€ ê³ ì¥ë‚¬ì–´\\nì•„. ê·¸ëŸ°ë°..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '../../data', 'raw_csv')\n",
    "train_data_path = os.path.join(data_dir, 'merged_train.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e0ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒ˜í”Œìˆ˜ : 4637\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4637 entries, 0 to 4636\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   idx           4637 non-null   int64 \n",
      " 1   class         4637 non-null   object\n",
      " 2   conversation  4637 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 108.8+ KB\n"
     ]
    }
   ],
   "source": [
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(train_data)))\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1511da3",
   "metadata": {},
   "source": [
    "**í•œêµ­ì–´ ë¶ˆìš©ì–´ ì‚¬ì „**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54171b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = ['ë‚˜', 'ë„ˆ', 'ë„¤', 'ë‚´','ì–´', 'ê³ ', 'ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c84dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì´', 'ìˆ', 'í•˜', 'ê²ƒ', 'ë“¤', 'ê·¸', 'ë˜', 'ìˆ˜', 'ì´', 'ë³´', 'ì•Š', 'ì—†', 'ë‚˜', 'ì‚¬ëŒ', 'ì£¼', 'ì•„ë‹ˆ', 'ë“±', 'ê°™', 'ìš°ë¦¬', 'ë•Œ', 'ë…„', 'ê°€', 'í•œ', 'ì§€', 'ëŒ€í•˜', 'ì˜¤', 'ë§', 'ì¼', 'ê·¸ë ‡', 'ìœ„í•˜', 'ë•Œë¬¸', 'ê·¸ê²ƒ', 'ë‘', 'ë§í•˜', 'ì•Œ', 'ê·¸ëŸ¬ë‚˜', 'ë°›', 'ëª»í•˜', 'ì¼', 'ê·¸ëŸ°', 'ë˜', 'ë¬¸ì œ', 'ë”', 'ì‚¬íšŒ', 'ë§', 'ê·¸ë¦¬ê³ ', 'ì¢‹', 'í¬', 'ë”°ë¥´', 'ì¤‘', 'ë‚˜ì˜¤', 'ê°€ì§€', 'ì”¨', 'ì‹œí‚¤', 'ë§Œë“¤', 'ì§€ê¸ˆ', 'ìƒê°í•˜', 'ê·¸ëŸ¬', 'ì†', 'í•˜ë‚˜', 'ì§‘', 'ì‚´', 'ëª¨ë¥´', 'ì ', 'ì›”', 'ë°', 'ìì‹ ', 'ì•ˆ', 'ì–´ë–¤', 'ë‚´', 'ê²½ìš°', 'ëª…', 'ìƒê°', 'ì‹œê°„', 'ê·¸ë…€', 'ë‹¤ì‹œ', 'ì´ëŸ°', 'ì•', 'ë³´ì´', 'ë²ˆ', 'ë‚˜', 'ë‹¤ë¥¸', 'ì–´ë–»', 'ì—¬ì', 'ê°œ', 'ë“¤', 'ì‚¬ì‹¤', 'ì´ë ‡', 'ì ', 'ì‹¶', 'ë§', 'ì •ë„', 'ì¢€', 'ì›', 'ì˜', 'í†µí•˜', 'ì†Œë¦¬', 'ë†“']\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/koreanStopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1afdd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z\\s]', '', text)  # í•œê¸€, ì˜ë¬¸, ê³µë°± ì œì™¸í•œ ë¬¸ì ì œê±°\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.strip()  # ì–‘ ë ê³µë°± ì œê±°\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workpalce = [ìƒì‚¬, ë¶€ì„œì¥, íŒ€ì¥, í›„ë°°, ìƒê¸‰ì, ë¶€í•˜ì§ì›, ì„ ë°°, ë™ë£Œ, ë¶€ì„œ, íŒ€, ë¶€ì„œì›, ê°„í˜¸ì‚¬, ê°„í˜¸ë¶€ì„œ, ê°„í˜¸ë‹¨ìœ„ ê´€ë¦¬ì]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bullying = [ì†Œë¦¬ ì§€ë¥´ë‹¤, í™”ë‚´ë‹¤, ì—…ë¬´ ë– ë„˜ê¸°ë‹¤, ì—…ë¬´ ì œì™¸, ì—…ë¬´ ê°ì‹œ, ì—…ë¬´ ê°„ì„­, ì°¨ë³„ ëŒ€ìš°, ë¹„í˜‘ì¡°, ë¹„ê¼¬ë‹¤, ëª¨ìš•í•˜ë‹¤, ìš•í•˜ë‹¤, ì•ˆ ì¢‹ì€ ì†Œë¬¸, ë¹„ë°©í•˜ë‹¤, í—˜ë‹´í•˜ë‹¤, ì˜ì‹¬í•˜ë‹¤, ì°¨ê°€ìš´ ë§íˆ¬, ì˜ì‹¬í•˜ëŠ” ëˆˆë¹›, ì°¨ë³„, ë¶ˆì´ìµ, ì••ë°•, ê°•ìš”, ì–µì••, ë°°ì œ, ë”°ëŒë¦¼, ë¬´ì‹œ, ë¬´ê´€ì‹¬ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0abd3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0131de1",
   "metadata": {},
   "source": [
    "**Mecab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "262fe6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒìœ„ ë‹¨ì–´\n",
      "[('í•˜', 9853), ('ì–´', 9167), ('ê³ ', 7436), ('ì•¼', 7180), ('ì•„', 5893), ('ê±°', 5489), ('ìˆ', 5208), ('ì§€', 5161), ('í•´', 4813), ('ì•ˆ', 4028), ('ê²Œ', 4003), ('ë‹¤', 3958), ('ì„', 3795), ('ë§', 3658), ('ë©´', 3509), ('ê² ', 3397), ('ì•„ë‹ˆ', 3214), ('ì—†', 2967), ('ì£¼', 2661), ('ë­', 2646), ('ì™œ', 2556), ('ë§Œ', 2501), ('ì•Œ', 2423), ('ì¢‹', 2386), ('ë‹ˆ', 2348), ('ìŠµë‹ˆë‹¤', 2151), ('ëˆ', 1997), ('ë‹˜', 1982), ('ì¼', 1934), ('ì£„ì†¡', 1892)]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Mecab : í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
    "#         í˜•íƒœì†Œ ì¶”ì¶œ(morphs), ëª…ì‚¬ ì¶”ì¶œ, í˜•íƒœì†Œì™€ íƒœê·¸ ì¶”ì¶œ \n",
    "tokenizer = Mecab()\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in df['conversation']:\n",
    "    clean_sentence = clean_text(sentence)\n",
    "    temp_X = tokenizer.morphs(clean_sentence) # í† í°í™”\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    all_tokens.append(temp_X)\n",
    "\n",
    "words = np.concatenate(all_tokens).tolist()\n",
    "#flattened_tokens = [token for sublist in all_tokens for token in sublist]\n",
    "    \n",
    "all_freqs = Counter(words).most_common(50)\n",
    "print(\"ì „ì²´ ìƒìœ„ ë‹¨ì–´\")\n",
    "print(all_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d1e0a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ëŠ”', 2166), ('ê³ ', 1770), ('ë„¤', 1591), ('ì—', 1276), ('ê±°', 1243), ('ë„', 1231), ('ì•¼', 1181), ('ìŠµë‹ˆë‹¤', 1119), ('ì£„ì†¡', 1055), ('ì•„', 1025), ('ì–´', 1008), ('ê² ', 994), ('í•´', 992), ('í•©ë‹ˆë‹¤', 983), ('ë‹˜', 981), ('ì€', 894), ('ê²Œ', 775), ('ë©´', 753), ('ì„', 743), ('ë‹¤', 739), ('ë„ˆ', 563), ('ì œ', 562), ('ì €', 554), ('ë­', 539), ('ì™œ', 484), ('ëŒ€ë¦¬', 448), ('íšŒì‚¬', 447), ('ëŠ”ë°', 442), ('í–ˆ', 441), ('í• ', 420), ('ëª»', 416), ('ì‹œ', 398), ('ë¥¼', 381), ('ë§Œ', 379), ('ìš”', 365), ('ì˜¤ëŠ˜', 345), ('ë¼ê³ ', 326), ('ë¡œ', 315), ('ê¹€', 310), ('ë¶€ì¥', 307), ('ìœ¼ë¡œ', 306), ('ë‹ˆ', 303), ('ì–´ìš”', 289), ('ë¬´ìŠ¨', 287), ('ì„œ', 283), ('ê·¸ëŸ¼', 275), ('ê¹Œì§€', 270), ('ê¸°', 253), ('ì„¸ìš”', 252), ('ë„ˆë¬´', 238)]\n",
      "\n",
      " [ì¼ë°˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ì–´', 3192), ('ì—', 1482), ('ëŠ”', 1406), ('ì•„', 1264), ('ì€', 969), ('í•´', 935), ('ê³ ', 863), ('ìš”ì¦˜', 821), ('ë„', 808), ('ì•¼', 691), ('ì„', 648), ('ë­', 624), ('ë‹¤', 575), ('ê² ', 497), ('ì •ë§', 412), ('ê²Œ', 403), ('ì–´ë–»ê²Œ', 395), ('ì˜¤ëŠ˜', 380), ('ì—ˆ', 375), ('ì´ë²ˆ', 353), ('ì£¼ë§', 351), ('ë„¤', 343), ('ì‘', 338), ('ë¨¹', 336), ('ì¬ë°Œ', 320), ('í–ˆ', 318), ('ì˜í™”', 312), ('ë¥¼', 295), ('í• ', 292), ('ì', 290), ('ê±°', 278), ('ì–´ë•Œ', 272), ('ê³ ë§ˆì›Œ', 272), ('ê³„íš', 271), ('ë´¤', 261), ('ë', 246), ('ì–´ë””', 235), ('ë©´', 224), ('ê°™ì´', 218), ('ì±…', 216), ('ì‹œ', 215), ('ë¡œ', 214), ('ì˜', 203), ('ì—ì„œ', 201), ('ì–´ìš”', 197), ('ì½', 196), ('ìŠµë‹ˆë‹¤', 196), ('ê·¸ê±°', 194), ('ì§€ë‚´', 193), ('ë§', 192)]\n",
      "\n",
      " [ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ì•¼', 1910), ('ëŠ”', 1904), ('ë„ˆ', 1552), ('ê³ ', 1538), ('ì•„', 1453), ('ì–´', 1414), ('ê±°', 1345), ('ë„', 1180), ('í•´', 1156), ('ë„¤', 1081), ('ì—', 912), ('ë‹¤', 905), ('ì™œ', 870), ('ê²Œ', 831), ('ë­', 679), ('ë‹ˆ', 678), ('ì€', 668), ('ë©´', 636), ('ëƒ', 625), ('ì„', 601), ('ê² ', 527), ('ë‹˜', 482), ('ëŠ”ë°', 480), ('ì§„ì§œ', 444), ('ë¼ê³ ', 419), ('ì„¸ìš”', 371), ('ë§Œ', 358), ('ë§ˆ', 356), ('ëª»', 344), ('í–ˆ', 332), ('ê·¸ë§Œ', 318), ('ê·¸ë˜', 317), ('í•œí…Œ', 314), ('ë´', 311), ('ì£„ì†¡', 306), ('ì–ì•„', 303), ('ê·¸ëƒ¥', 296), ('ì• ', 290), ('ë¬´ìŠ¨', 289), ('ì‹œ', 289), ('í•©ë‹ˆë‹¤', 281), ('ë¥¼', 276), ('ì„œ', 270), ('í• ', 265), ('ë„ˆë¬´', 261), ('ì €', 257), ('ê¸°', 241), ('ë¯¸ì•ˆ', 240), ('ë§', 237), ('ì–´ìš”', 231)]\n",
      "\n",
      " [í˜‘ë°• ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ì–´', 1859), ('ëŠ”', 1855), ('ì•¼', 1777), ('ê³ ', 1707), ('ë„ˆ', 1586), ('ê±°', 1483), ('ì—', 1135), ('ì„', 1127), ('ë„', 1097), ('í•´', 1047), ('ê²Œ', 1042), ('ì•„', 1011), ('ë©´', 973), ('ë„¤', 966), ('ë‹¤', 863), ('ê² ', 810), ('ë‹ˆ', 804), ('ì€', 746), ('ì£½', 711), ('ì™œ', 623), ('ë§Œ', 618), ('ì„¸ìš”', 579), ('í–ˆ', 528), ('ë¥¼', 516), ('í• ', 477), ('ì œë°œ', 460), ('ëª»', 447), ('ë¡œ', 423), ('ë­', 416), ('ìŠµë‹ˆë‹¤', 395), ('ì£„ì†¡', 355), ('ë¼ê³ ', 349), ('ì§„ì§œ', 340), ('ëŠ”ë°', 307), ('ìœ¼ë©´', 299), ('ì‹œ', 299), ('í•©ë‹ˆë‹¤', 297), ('ê¸°', 295), ('í•œí…Œ', 291), ('ì', 287), ('ìœ¼ë¡œ', 286), ('ê·¸ë˜', 284), ('ëƒ', 282), ('ë¬´ìŠ¨', 277), ('ê¹Œì§€', 271), ('ë´', 268), ('ì œ', 267), ('ì €', 258), ('ì„œ', 255), ('ê·¸ëƒ¥', 250)]\n",
      "\n",
      " [ê°ˆì·¨ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ì–´', 1694), ('ì•¼', 1621), ('ê³ ', 1558), ('ëˆ', 1522), ('ëŠ”', 1405), ('ì•„', 1140), ('ê±°', 1140), ('ì—', 1129), ('ë„¤', 1067), ('ë§Œ', 1052), ('ë„ˆ', 998), ('ë„', 955), ('ê²Œ', 952), ('ë©´', 923), ('ë‹¤', 876), ('ì€', 700), ('í•´', 683), ('ì„', 676), ('ê² ', 569), ('ì™œ', 560), ('ì„¸ìš”', 537), ('ì–´ìš”', 464), ('ë‹ˆ', 463), ('ëŠ”ë°', 459), ('ì§„ì§œ', 455), ('ì´ê±°', 415), ('ìš”', 413), ('ëƒ', 405), ('ì €', 397), ('ê·¸ëŸ¼', 389), ('ë­', 388), ('ì‹œ', 375), ('í•œí…Œ', 360), ('ë¼', 345), ('ì¤˜', 340), ('ë¹Œë ¤', 338), ('ê·¸ë˜', 321), ('ë¡œ', 321), ('ë´', 313), ('ì‚¬', 308), ('í• ', 302), ('ë§', 283), ('ë‹˜', 283), ('ì–ì•„', 281), ('ì œ', 273), ('ë‚´ë†”', 268), ('ë¼', 257), ('ê·¸ëƒ¥', 251), ('ë¼ê³ ', 249), ('ëª»', 249)]\n"
     ]
    }
   ],
   "source": [
    "# í´ë˜ìŠ¤ë³„ ë¹ˆë„ ê³„ì‚°\n",
    "class_freqs = {}\n",
    "for label in df['class'].unique():\n",
    "    class_tokens = []\n",
    "    sub_df = df[df['class'] == label]\n",
    "    for conv in sub_df['conversation']:\n",
    "        clean_sentence = clean_text(conv)\n",
    "        tokens = tokenizer.morphs(clean_sentence) # í† í°í™”\n",
    "        tokens = [word for word in tokens if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "        class_tokens.extend(tokens)\n",
    "    \n",
    "    class_freqs[label] = Counter(class_tokens).most_common(50)\n",
    "\n",
    "for key, value in class_freqs.items():\n",
    "    print(f\"\\n [{key}] ìƒìœ„ ë‹¨ì–´:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3b0f1",
   "metadata": {},
   "source": [
    "**Okt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ddc53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# í’ˆì‚¬ íƒœê¹…\n",
    "def pos_tagging(text):\n",
    "    # ê´€í˜•ì‚¬, ê°íƒ„ì‚¬, ì¡°ì‚¬, ì–´ë¯¸ ë“±ì€ ë¶ˆìš©ì–´ ëŒ€ìƒìœ¼ë¡œ í† í°í™” ê³¼ì •ì— ì œì™¸\n",
    "    # ë¶€ì‚¬ëŠ” ìš°ë¦¬ì˜ taskì— ì¤‘ìš”í•œ ì •ë³´ë¼ íŒë‹¨í•˜ì—¬ í¬í•¨(\"ì§„ì§œ ì§œì¦ë‚˜\", \"ë„ˆ ì •ë§ ëª»ëë‹¤\")\n",
    "    return [word for word, pos in okt.pos(text) if pos in ['Noun', 'Verb', 'Adjective', 'Adverb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "790ccada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒìœ„ ë‹¨ì–´\n",
      "[('ë„ˆ', 4622), ('ë‚´', 3896), ('ë‚˜', 2720), ('ë§', 2640), ('ë„¤', 2599), ('ì™œ', 2485), ('ë­', 2377), ('ê±°', 2262), ('ì¢€', 2073), ('ëˆ', 1930), ('í•´', 1752), ('ë‹¤', 1750), ('ìˆì–´', 1583), ('ê²ƒ', 1513), ('ì´', 1481), ('ì§„ì§œ', 1470), ('ì£„ì†¡í•©ë‹ˆë‹¤', 1425), ('ì €', 1412), ('ì§€ê¸ˆ', 1407), ('ë‹ˆ', 1402), ('ì•„ë‹ˆ', 1364), ('ì œ', 1263), ('ê·¸ëŸ¼', 1252), ('ê·¸ë˜', 1215), ('ì˜¤ëŠ˜', 1157), ('ìš°ë¦¬', 1112), ('ì•ˆ', 1109), ('ìƒê°', 1094), ('í• ', 1049), ('ìš”ì¦˜', 1043)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ì „ì²´ í’ˆì‚¬ë³„ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "all_tokens = []\n",
    "for conv in df['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(30)\n",
    "print(\"ì „ì²´ ìƒìœ„ ë‹¨ì–´\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c02e6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜‘ë°• ìƒìœ„ ë‹¨ì–´\n",
      "[('ë„ˆ', 1506), ('ì™œ', 599), ('ë‹ˆ', 570), ('ë‹¤', 492), ('ì œë°œ', 461), ('í•´', 433), ('ë„¤', 409), ('ê±°', 392), ('ë­', 342), ('ì§„ì§œ', 338), ('ê·¸ë˜', 286), ('ì œ', 252), ('ê·¸ëƒ¥', 248), ('ì£½ì—¬', 248), ('ì£„ì†¡í•©ë‹ˆë‹¤', 240), ('ì €', 233), ('ëˆ', 228), ('í• ', 225), ('ê·¸ëŸ¼', 218), ('í•˜ë©´', 217), ('ì •ë§', 202), ('ì–´ë–»ê²Œ', 201), ('ë¬´ìŠ¨', 199), ('ìƒˆë¼', 195), ('ëª»', 185), ('ë‹¹ì‹ ', 172), ('ë´', 171), ('ê°€ì¡±', 170), ('ìˆì–´', 165), ('ë‚œ', 164), ('ì¹¼', 164), ('ì—¬ê¸°', 163), ('ë„Œ', 161), ('ê·¸ë ‡ê²Œ', 161), ('ì‹ ê³ ', 161), ('ì•„ë‹ˆì•¼', 161), ('ë‹¹ì¥', 157), ('ì´ì œ', 146), ('ì‚´ë ¤ì£¼ì„¸ìš”', 143), ('ë”¸', 141), ('ì• ', 137), ('ì–´ë””', 133), ('ì£½', 130), ('ë¹¨ë¦¬', 129), ('í•œë²ˆ', 129), ('ë‚˜ë„', 127), ('ì´ë ‡ê²Œ', 125), ('í•˜ì§€', 122), ('ë¼', 121), ('ì£½ì–´', 120), ('ì „', 120), ('ì¤„', 119), ('ì—†ì–´', 118), ('ê²½ì°°', 117), ('í•˜ëŠ”', 116), ('ê·¸ê²Œ', 116), ('ë­˜', 114), ('ë‚ ', 113), ('ì˜¤ëŠ˜', 112), ('ë²„ë¦´ê±°ì•¼', 112), ('ê²Œ', 101), ('ì •ì‹ ', 100), ('ëˆˆ', 98), ('í–ˆì–´', 97), ('ê·¸ê±´', 95), ('ê±¸', 94), ('ì…', 93), ('ë¯¸ì•ˆí•´', 93), ('ê°™ì•„', 92), ('ë„ˆë¬´', 91), ('ê±´', 90), ('ê·¸ëŸ¬ë©´', 87), ('ëˆ„êµ¬', 86), ('í•˜ê² ìŠµë‹ˆë‹¤', 86), ('ì¹œêµ¬', 83), ('ìê¾¸', 82), ('ê°™ì´', 81), ('í˜‘ë°•', 81), ('ì—„ë§ˆ', 80), ('ë†ˆ', 79), ('í•´ë´', 79), ('ëª©ìˆ¨', 78), ('ë™ìƒ', 77), ('ì•„ë“¤', 77), ('ì£½ê³ ', 76), ('ì´ë²ˆ', 75), ('ì¥ê¸°', 74), ('ì˜ëª»', 73), ('ê°™ì€', 73), ('ë„', 73), ('ì†', 72), ('ìˆëŠ”', 72), ('ì¡°ê¸ˆ', 72), ('ë§ë¡œ', 71), ('ì•„ì´', 70), ('ì£„ì†¡í•´ìš”', 69), ('ì•„ì£¼', 67), ('ì–˜ê¸°', 67), ('ì˜¤ë¹ ', 67), ('ëŒ€ë¡œ', 66)]\n"
     ]
    }
   ],
   "source": [
    "# í˜‘ë°• ëŒ€í™” ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "subset = df[ (df['class'] == \"í˜‘ë°• ëŒ€í™”\") ]\n",
    "all_tokens = []\n",
    "\n",
    "for conv in subset['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(100)\n",
    "print(\"í˜‘ë°• ìƒìœ„ ë‹¨ì–´\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76dbee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê´´ë¡­í˜ ìƒìœ„ ë‹¨ì–´\n",
      "[('ì™œ', 1327), ('ë§', 1285), ('ì£„ì†¡í•©ë‹ˆë‹¤', 1095), ('ë­', 1059), ('ê±°', 984), ('ì•„ë‹ˆ', 782), ('í•´', 751), ('ì €', 750), ('ì œ', 730), ('ë‹¤', 712), ('ì§„ì§œ', 629), ('ì¼', 628), ('ì§€ê¸ˆ', 618), ('ê²ƒ', 590), ('ë‹ˆ', 547), ('ìš°ë¦¬', 542), ('ê·¸ë˜', 538), ('ì•ˆ', 531), ('ì‚¬ëŒ', 485), ('ê·¸ëŸ¼', 479), ('ê·¸ëƒ¥', 470), ('ì˜¤ëŠ˜', 462), ('ë´', 457), ('ëŒ€ë¦¬', 457), ('íšŒì‚¬', 456), ('ê·¸ë ‡ê²Œ', 447), ('ë¬´ìŠ¨', 431), ('í• ', 403), ('ë„ˆë¬´', 400), ('í•˜ëŠ”', 364), ('ì•Œ', 355), ('ì• ', 344), ('ìƒê°', 337), ('ë¶€ì¥', 317), ('ì´ë ‡ê²Œ', 311), ('ì—¬ê¸°', 307), ('ë”', 307), ('ê·¸', 298), ('ëª»', 293), ('ë•Œ', 286), ('ìƒˆë¼', 281), ('ì•„ë‹ˆì•¼', 280), ('ê¹€', 279), ('ì´ë²ˆ', 274), ('ì–´ë–»ê²Œ', 273), ('ì§‘', 270), ('ì‹œê°„', 270), ('ê·¸ê²Œ', 263), ('í•˜ë©´', 261), ('í•˜ë‚˜', 256), ('ë§ì”€', 253), ('ìš”', 246), ('ë„Œ', 245), ('ì•„ë‹™ë‹ˆë‹¤', 241), ('ë­˜', 239), ('ìë„¤', 237), ('ë¹¨ë¦¬', 237), ('í•˜ê² ìŠµë‹ˆë‹¤', 237), ('ëˆ', 234), ('ê·¸ê±´', 233), ('ì‘', 231), ('í•˜ì§€', 229), ('ê³¼ì¥', 228), ('í•˜ê³ ', 221), ('íŒ€', 219), ('í•´ì„œ', 219), ('ìˆ˜', 219), ('ì •ë§', 215), ('ê·¸ëŸ°', 214), ('ì¹œêµ¬', 207), ('ë‚´ì¼', 204), ('ê·¸ë˜ë„', 204), ('ê±¸', 204), ('ë‹¤ì‹œ', 203), ('ê·¼ë°', 202), ('ì•', 201), ('ì—…ë¬´', 191), ('ì¤„', 187), ('ê²Œ', 186), ('ì—†ì–´', 184), ('ì•Œê² ìŠµë‹ˆë‹¤', 183), ('ì œë°œ', 181), ('ì–˜', 180), ('ì—„ë§ˆ', 179), ('ë‹¤ë¥¸', 176), ('ê°™ì€', 176), ('ìˆì–´', 175), ('ê³ ê°', 175), ('ë‚œ', 171), ('ì–¼êµ´', 171), ('íœ´ê°€', 170), ('ê°™ì´', 170), ('ë¼', 168), ('ê·¸ê±°', 168), ('ê¸°ë¶„', 160), ('ì €ê¸°', 159), ('ì–´ë””', 157), ('ì†ë‹˜', 157), ('í–ˆì–´', 155), ('ì˜·', 154)]\n"
     ]
    }
   ],
   "source": [
    "# ê´´ë¡­í˜ ëŒ€í™” ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "subset = df[ (df['class'] == \"ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”\") | (df['class'] == \"ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”\")]\n",
    "all_tokens = []\n",
    "\n",
    "for conv in subset['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(100)\n",
    "print(\"ê´´ë¡­í˜ ìƒìœ„ ë‹¨ì–´\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64b17def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ë„¤', 1095), ('ì£„ì†¡í•©ë‹ˆë‹¤', 871), ('ë„ˆ', 542), ('ì œ', 528), ('ì €', 509), ('ê±°', 483), ('ë­', 482), ('ì™œ', 478), ('ëŒ€ë¦¬', 455), ('íšŒì‚¬', 447), ('í•´', 418), ('ë‹¤', 413), ('ì˜¤ëŠ˜', 344), ('ë¶€ì¥', 316), ('ê·¸ëŸ¼', 274), ('ê¹€', 272), ('í• ', 260), ('ê·¸ë˜', 233), ('ê³¼ì¥', 228), ('ìë„¤', 228), ('ë¬´ìŠ¨', 221), ('ì•„ë‹™ë‹ˆë‹¤', 215), ('í•˜ê² ìŠµë‹ˆë‹¤', 212), ('í•˜ëŠ”', 211), ('íŒ€', 210), ('ì´ë²ˆ', 210), ('ê·¸ë ‡ê²Œ', 207), ('ë„ˆë¬´', 195), ('ì—…ë¬´', 188), ('ë´', 187), ('ì§„ì§œ', 185), ('ë§ì”€', 184), ('ì´ë ‡ê²Œ', 180), ('ê·¸ëƒ¥', 175), ('í•˜ë©´', 173), ('ë‚´ì¼', 170), ('íœ´ê°€', 168), ('ìš”', 164), ('ëª»', 161), ('ì•Œê² ìŠµë‹ˆë‹¤', 158), ('ê·¸ê²Œ', 152), ('ê·¸ê±´', 145), ('í•´ì„œ', 142), ('í•˜ê³ ', 141), ('ê·¸ë˜ë„', 135), ('ì»¤í”¼', 135), ('ì–´ë–»ê²Œ', 131), ('ë¹¨ë¦¬', 129), ('ë‹ˆ', 129), ('ì •ë§', 129)]\n",
      "\n",
      " [ì¼ë°˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ìˆì–´', 1100), ('ìš”ì¦˜', 821), ('ë­', 623), ('ì •ë§', 412), ('ê°™ì•„', 407), ('ì–´ë–»ê²Œ', 396), ('ì˜¤ëŠ˜', 380), ('ì¢‹ì€', 378), ('ì´ë²ˆ', 353), ('ì£¼ë§', 351), ('í•´', 340), ('ì‘', 338), ('ì˜í™”', 309), ('ë‚˜ë„', 304), ('ì–´ë•Œ', 273), ('ê³ ë§ˆì›Œ', 272), ('ê³„íš', 271), ('ì¢‹ì•„', 261), ('í–ˆì–´', 247), ('í• ', 242), ('ëì–´', 229), ('ê°™ì´', 218), ('ì±…', 218), ('ì–´ë• ì–´', 200), ('ì–´ë””', 194), ('ê·¸ê±°', 194), ('ì¢‹ì•„í•´', 190), ('ì¶”ì²œ', 177), ('ì§€ë‚´', 175), ('ì¬ë°ŒëŠ”', 175), ('ê·¸ëŸ¼', 172), ('ìƒˆë¡œ', 169), ('ë„¤', 163), ('ì €ë…', 161), ('ê±°', 160), ('ë‚ ì”¨', 159), ('ë‹¤ìŒ', 155), ('ì¤€ë¹„', 153), ('ì‹¶ì–´', 147), ('ì»¤í”¼', 145), ('ì‚°', 143), ('ì‹œ', 139), ('ë´¤ì–´', 139), ('ë„ˆ', 137), ('ê·¸ë ‡ê²Œ', 136), ('ìŒì•…', 136), ('ì¹œêµ¬', 131), ('ê·¸ë˜ì„œ', 131), ('í• ë˜', 130), ('ë³¸', 129)]\n",
      "\n",
      " [ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ë„ˆ', 1477), ('ì™œ', 849), ('ë­', 577), ('ê±°', 501), ('ì§„ì§œ', 444), ('ë‹ˆ', 418), ('ë„¤', 395), ('í•´', 333), ('ê·¸ë˜', 305), ('ë‹¤', 299), ('ê·¸ëƒ¥', 295), ('ë´', 270), ('ì €', 241), ('ê·¸ë ‡ê²Œ', 240), ('ì• ', 230), ('ì£„ì†¡í•©ë‹ˆë‹¤', 224), ('ì•„ë‹ˆì•¼', 212), ('ë¬´ìŠ¨', 210), ('ê·¸ëŸ¼', 205), ('ë„ˆë¬´', 205), ('ì œ', 202), ('ì—¬ê¸°', 200), ('ì‘', 195), ('ìƒˆë¼', 192), ('ê³ ê°', 173), ('ì—„ë§ˆ', 168), ('ë„Œ', 163), ('ì–˜', 160), ('ì¹œêµ¬', 159), ('ëˆ', 154), ('í•˜ëŠ”', 153), ('ë­˜', 149), ('ì œë°œ', 149), ('ì†ë‹˜', 146), ('í• ', 143), ('ì–´ë–»ê²Œ', 142), ('ëƒ„ìƒˆ', 139), ('ëª»', 132), ('ë‚œ', 132), ('ì´ë ‡ê²Œ', 131), ('ì–¼êµ´', 131), ('í•˜ì§€', 129), ('ê·¸ë§Œí•´', 128), ('í™˜ë¶ˆ', 121), ('ì˜¤ëŠ˜', 118), ('í•˜ì§€ë§ˆ', 116), ('ê¸°ë¶„', 116), ('ê·¼ë°', 112), ('ë¯¸ì•ˆí•´', 112), ('ê·¸ê²Œ', 111)]\n",
      "\n",
      " [í˜‘ë°• ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ë„ˆ', 1506), ('ì™œ', 599), ('ë‹ˆ', 570), ('ë‹¤', 492), ('ì œë°œ', 461), ('í•´', 433), ('ë„¤', 409), ('ê±°', 392), ('ë­', 342), ('ì§„ì§œ', 338), ('ê·¸ë˜', 286), ('ì œ', 252), ('ê·¸ëƒ¥', 248), ('ì£½ì—¬', 248), ('ì£„ì†¡í•©ë‹ˆë‹¤', 240), ('ì €', 233), ('ëˆ', 228), ('í• ', 225), ('ê·¸ëŸ¼', 218), ('í•˜ë©´', 217), ('ì •ë§', 202), ('ì–´ë–»ê²Œ', 201), ('ë¬´ìŠ¨', 199), ('ìƒˆë¼', 195), ('ëª»', 185), ('ë‹¹ì‹ ', 172), ('ë´', 171), ('ê°€ì¡±', 170), ('ìˆì–´', 165), ('ë‚œ', 164), ('ì¹¼', 164), ('ì—¬ê¸°', 163), ('ë„Œ', 161), ('ê·¸ë ‡ê²Œ', 161), ('ì‹ ê³ ', 161), ('ì•„ë‹ˆì•¼', 161), ('ë‹¹ì¥', 157), ('ì´ì œ', 146), ('ì‚´ë ¤ì£¼ì„¸ìš”', 143), ('ë”¸', 141), ('ì• ', 137), ('ì–´ë””', 133), ('ì£½', 130), ('ë¹¨ë¦¬', 129), ('í•œë²ˆ', 129), ('ë‚˜ë„', 127), ('ì´ë ‡ê²Œ', 125), ('í•˜ì§€', 122), ('ë¼', 121), ('ì£½ì–´', 120)]\n",
      "\n",
      " [ê°ˆì·¨ ëŒ€í™”] ìƒìœ„ ë‹¨ì–´:\n",
      "[('ëˆ', 1466), ('ë„ˆ', 960), ('ê±°', 726), ('ì™œ', 540), ('ë„¤', 537), ('ì§„ì§œ', 458), ('ë‹¤', 456), ('ë§Œì›', 401), ('ê·¸ëŸ¼', 383), ('ì €', 380), ('ë­', 353), ('ë¼', 334), ('ê·¸ë˜', 316), ('ë´', 288), ('ë‹ˆ', 284), ('ì—†ì–´', 268), ('ë‚´ë†”', 267), ('ì œ', 261), ('ê·¸ëƒ¥', 241), ('ì œë°œ', 235), ('ì¤„', 228), ('í•´', 228), ('ì—†ì–´ìš”', 227), ('ì—¬ê¸°', 206), ('ì˜¤ëŠ˜', 203), ('ì´ë²ˆ', 203), ('ë¹¨ë¦¬', 203), ('ì—„ë§ˆ', 194), ('ì¤˜', 188), ('ì¹œêµ¬', 187), ('í• ', 179), ('ë‚˜ë„', 173), ('ë¬´ìŠ¨', 169), ('ì •ë§', 156), ('ê·¸ê±´', 156), ('ê·¸ê±°', 150), ('ì‘', 147), ('ìˆì–´', 143), ('ì–´ë–»ê²Œ', 143), ('ìƒˆë¼', 140), ('í•œë²ˆ', 121), ('ì–¼ë§ˆ', 119), ('ì €ë²ˆ', 116), ('ë¼ìš”', 114), ('ë¹Œë ¤', 112), ('ê·¼ë°', 111), ('í•˜ë©´', 111), ('ê·¸ê²Œ', 111), ('ì•„ë‹ˆì•¼', 110), ('ë‚´ì¼', 109)]\n"
     ]
    }
   ],
   "source": [
    "# í´ë˜ìŠ¤ë³„ ë¹ˆë„ ê³„ì‚°\n",
    "class_freqs = {}\n",
    "for label in df['class'].unique():\n",
    "    class_tokens = []\n",
    "    sub_df = df[df['class'] == label]\n",
    "    for conv in sub_df['conversation']:\n",
    "        tokens = pos_tagging(clean_text(conv))\n",
    "        tokens = [word for word in tokens if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "        class_tokens.extend(tokens)\n",
    "        \n",
    "    class_freqs[label] = Counter(class_tokens).most_common(50)\n",
    "\n",
    "for key, value in class_freqs.items():\n",
    "    print(f\"\\n [{key}] ìƒìœ„ ë‹¨ì–´:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a7a57",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "\n",
    "ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ëª¨ë“  ë¬¸ì„œì— ë‹¤ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë‚®ê²Œ,\n",
    "íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë†’ê²Œ ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b8105b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['conversation'].apply(lambda x: ' '.join(pos_tagging(clean_text(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cd3c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ê°€ê°€' 'ê°€ê°' 'ê°€ê±°ë“ ìš”' ... 'í˜ì¢€' 'í™ë‹ˆ' 'í™í•©']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF ë²¡í„°í™”\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['tokenized'])\n",
    "\n",
    "# ë‹¨ì–´ ëª©ë¡\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF í–‰ë ¬ í™•ì¸\n",
    "print(X_tfidf.toarray())\n",
    "\n",
    "#print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3abbec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³¼ì¥: 0.2091\n",
      "ë„‰ë„‰í•˜ê²Œ: 0.1926\n",
      "ëŠ¦ì–´ì§€ê²Œ: 0.2020\n",
      "ëê³ : 0.1084\n",
      "ë ê¹Œ: 0.1183\n",
      "ë©ë‹ˆë‹¤: 0.1277\n",
      "ë§ˆë¬´ë¦¬: 0.3919\n",
      "ë¬´ë¦¬í•˜ì§€: 0.1926\n",
      "ë°›ì•˜ì–´ìš”: 0.1860\n",
      "ë³´ë‚´: 0.1199\n",
      "ìƒê²¼ë„¤: 0.1606\n",
      "ì•„ë‡¨: 0.1247\n",
      "ì•„ë‹ˆ: 0.0582\n",
      "ì•„ì§: 0.1823\n",
      "ì•Œê² ìŠµë‹ˆë‹¤: 0.0874\n",
      "ì–´ë–¡í•©ë‹ˆê¹Œ: 0.1766\n",
      "ì–¸ì œ: 0.0904\n",
      "ì—†ë”ë¼êµ¬ìš”: 0.2020\n",
      "ì˜¤ëŠ˜: 0.1942\n",
      "ìë¦¬: 0.1114\n",
      "ì‘ì„±: 0.1312\n",
      "ì¥ë‹˜: 0.2257\n",
      "ì „ë‹¬: 0.2821\n",
      "ì£„ì†¡í•´ìš”: 0.1010\n",
      "ì£¼ë§: 0.1725\n",
      "í‡´ê·¼: 0.1136\n",
      "í•˜ë¼: 0.1014\n",
      "í•˜ë©´: 0.1430\n",
      "í•˜ì…”ì„œ: 0.1353\n",
      "í•´ë‹¬ë¼: 0.1860\n",
      "í•´ì„œ: 0.0782\n",
      "í•´ì£¼ì‹œë©´: 0.1477\n",
      "í–ˆë‚˜ìš”: 0.1446\n",
      "í–ˆëŠ”ë°: 0.1874\n",
      "í–ˆì–´ìš”: 0.1121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0ë²ˆì§¸ ë¬¸ì¥ì˜ ë¹„ì–´ìˆì§€ ì•Šì€ ë‹¨ì–´ë§Œ í™•ì¸\n",
    "row_0 = X_tfidf[0].toarray().flatten()\n",
    "non_zero_indices = np.where(row_0 > 0)[0]\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in non_zero_indices:\n",
    "    print(f\"{words[i]}: {row_0[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca696780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ í´ë˜ìŠ¤: ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”\n",
      "ì£„ì†¡í•©ë‹ˆë‹¤: 0.0404\n",
      "ë‚´ê°€: 0.0254\n",
      "ì•„ë‹ˆ: 0.0210\n",
      "ì œê°€: 0.0197\n",
      "ì§€ê¸ˆ: 0.0191\n",
      "ê·¸ëŸ¼: 0.0167\n",
      "ë¶€ì¥ë‹˜: 0.0167\n",
      "ì˜¤ëŠ˜: 0.0160\n",
      "ì•„ë‹™ë‹ˆë‹¤: 0.0159\n",
      "ë¬´ìŠ¨: 0.0141\n",
      "ì•Œê² ìŠµë‹ˆë‹¤: 0.0133\n",
      "ê·¸ë˜: 0.0130\n",
      "ê·¸ë ‡ê²Œ: 0.0128\n",
      "ë„ˆë¬´: 0.0125\n",
      "ì§„ì§œ: 0.0120\n",
      "ê¹€ëŒ€ë¦¬: 0.0118\n",
      "ê·¸ëƒ¥: 0.0117\n",
      "ì´ê±°: 0.0112\n",
      "íšŒì‚¬: 0.0111\n",
      "ì´ë ‡ê²Œ: 0.0111\n",
      "ê³¼ì¥ë‹˜: 0.0109\n",
      "ê·¸ê²Œ: 0.0108\n",
      "ìë„¤: 0.0106\n",
      "ë‹¤ì‹œ: 0.0104\n",
      "ê·¸ê±´: 0.0101\n",
      "ì¼ì´: 0.0101\n",
      "ì œëŒ€ë¡œ: 0.0099\n",
      "ì •ë§: 0.0098\n",
      "ìš°ë¦¬: 0.0098\n",
      "í•˜ê³ : 0.0098\n",
      "ê·¸ë˜ë„: 0.0097\n",
      "ì–´ë–»ê²Œ: 0.0094\n",
      "ì¼ì„: 0.0092\n",
      "í•˜ê² ìŠµë‹ˆë‹¤: 0.0090\n",
      "ë‚´ì¼: 0.0088\n",
      "ë¹¨ë¦¬: 0.0088\n",
      "ë‹¤ë¥¸: 0.0084\n",
      "ê°™ìŠµë‹ˆë‹¤: 0.0083\n",
      "ìš”ì¦˜: 0.0082\n",
      "ì•„ë‹ˆìš”: 0.0081\n",
      "ì €ë„: 0.0080\n",
      "íœ´ê°€: 0.0079\n",
      "ê·¸ëŸ°: 0.0078\n",
      "ê°™ì´: 0.0077\n",
      "ì´ê±´: 0.0075\n",
      "íŒ€ì¥ë‹˜: 0.0074\n",
      "ì—†ì–´: 0.0073\n",
      "ì‚¬ëŒì´: 0.0072\n",
      "ê·¼ë°: 0.0072\n",
      "í•˜ëŠ”: 0.0070\n",
      "\n",
      "ğŸ“Œ í´ë˜ìŠ¤: ì¼ë°˜ ëŒ€í™”\n",
      "ìˆì–´: 0.0653\n",
      "ìš”ì¦˜: 0.0566\n",
      "ì •ë§: 0.0294\n",
      "ì–´ë–»ê²Œ: 0.0292\n",
      "ê°™ì•„: 0.0289\n",
      "ì˜¤ëŠ˜: 0.0281\n",
      "ì¢‹ì€: 0.0276\n",
      "ì£¼ë§ì—: 0.0246\n",
      "ì˜í™”: 0.0246\n",
      "ì´ë²ˆ: 0.0243\n",
      "ì–´ë–¤: 0.0239\n",
      "ë‚˜ë„: 0.0235\n",
      "ì–´ë•Œ: 0.0230\n",
      "ì¢‹ì•„: 0.0225\n",
      "ê³ ë§ˆì›Œ: 0.0216\n",
      "ê°™ì´: 0.0199\n",
      "ëì–´: 0.0195\n",
      "ì–´ë• ì–´: 0.0185\n",
      "ê·¸ëŸ¼: 0.0183\n",
      "ì‹¶ì–´: 0.0183\n",
      "ì¢‹ì•„í•´: 0.0179\n",
      "ì‹œê°„: 0.0179\n",
      "ê·¸ê±°: 0.0175\n",
      "ì§€ë‚´: 0.0164\n",
      "ì¬ë°ŒëŠ”: 0.0162\n",
      "ë‚˜ëŠ”: 0.0157\n",
      "ê±°ì•¼: 0.0155\n",
      "ìƒˆë¡œ: 0.0153\n",
      "ê³„íš: 0.0134\n",
      "ê·¸ë ‡ê²Œ: 0.0134\n",
      "ìƒê°ì´ì•¼: 0.0133\n",
      "ë´¤ì–´: 0.0133\n",
      "ì»¤í”¼: 0.0133\n",
      "ê·¸ë˜ì„œ: 0.0131\n",
      "ì €ë…: 0.0130\n",
      "ë§ì•„: 0.0126\n",
      "ìƒê°í•´: 0.0125\n",
      "ìˆëŠ”: 0.0125\n",
      "ìˆë‚˜ìš”: 0.0124\n",
      "ì–´ì œ: 0.0122\n",
      "ìŒì•…: 0.0119\n",
      "ì–´ë””: 0.0118\n",
      "ìµœê·¼ì—: 0.0118\n",
      "ë‚ ì”¨: 0.0118\n",
      "ìƒˆë¡œìš´: 0.0117\n",
      "í•œì”í• ë˜: 0.0115\n",
      "í–ˆì–´: 0.0115\n",
      "ë„ˆë¬´: 0.0115\n",
      "í•¨ê»˜: 0.0111\n",
      "ì½ê³ : 0.0107\n",
      "\n",
      "ğŸ“Œ í´ë˜ìŠ¤: ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”\n",
      "ë‚´ê°€: 0.0277\n",
      "ì§„ì§œ: 0.0224\n",
      "ì•„ë‹ˆ: 0.0197\n",
      "ê·¸ëƒ¥: 0.0174\n",
      "ì£„ì†¡í•©ë‹ˆë‹¤: 0.0160\n",
      "ê·¸ë˜: 0.0157\n",
      "ì•„ë‹ˆì•¼: 0.0151\n",
      "ë‹ˆê°€: 0.0138\n",
      "ì§€ê¸ˆ: 0.0137\n",
      "ê·¸ë ‡ê²Œ: 0.0136\n",
      "ê·¸ëŸ¼: 0.0135\n",
      "ë¬´ìŠ¨: 0.0133\n",
      "ë„ˆë¬´: 0.0130\n",
      "ê·¸ë§Œí•´: 0.0120\n",
      "ì œë°œ: 0.0117\n",
      "ìš°ë¦¬: 0.0113\n",
      "ì´ê±°: 0.0111\n",
      "ë„ˆê°€: 0.0111\n",
      "ë¯¸ì•ˆí•´: 0.0108\n",
      "ê³ ê°ë‹˜: 0.0107\n",
      "ì—¬ê¸°: 0.0106\n",
      "ì œê°€: 0.0099\n",
      "ì†ë‹˜: 0.0097\n",
      "ì–´ë–»ê²Œ: 0.0096\n",
      "ë‚˜ë„: 0.0085\n",
      "ê·¸ë§Œ: 0.0084\n",
      "ê·¸ê²Œ: 0.0083\n",
      "ì´ë ‡ê²Œ: 0.0082\n",
      "ë§ì´: 0.0082\n",
      "ë„ˆëŠ”: 0.0082\n",
      "ê·¼ë°: 0.0081\n",
      "ë¯¸ì•ˆ: 0.0081\n",
      "ë¹¨ë¦¬: 0.0081\n",
      "í•˜ì§€ë§ˆ: 0.0080\n",
      "ê·¸ëŸ°: 0.0080\n",
      "ë‚˜í•œí…Œ: 0.0080\n",
      "ë­ê°€: 0.0078\n",
      "ë‚˜ëŠ”: 0.0075\n",
      "ë­ì•¼: 0.0073\n",
      "ì˜¤ëŠ˜: 0.0072\n",
      "ì´ê²Œ: 0.0072\n",
      "ì‹«ì–´: 0.0072\n",
      "ê·¸ê±´: 0.0069\n",
      "ì—„ë§ˆ: 0.0068\n",
      "ì •ë§: 0.0068\n",
      "ì—†ì–´: 0.0067\n",
      "ìˆì–´: 0.0064\n",
      "ì´ì œ: 0.0063\n",
      "ì™œê·¸ë˜: 0.0062\n",
      "ë‹¤ì‹œ: 0.0060\n",
      "\n",
      "ğŸ“Œ í´ë˜ìŠ¤: í˜‘ë°• ëŒ€í™”\n",
      "ë‚´ê°€: 0.0372\n",
      "ì œë°œ: 0.0250\n",
      "ì§„ì§œ: 0.0207\n",
      "ì£„ì†¡í•©ë‹ˆë‹¤: 0.0206\n",
      "ì§€ê¸ˆ: 0.0202\n",
      "ë‹ˆê°€: 0.0165\n",
      "ê·¸ëƒ¥: 0.0162\n",
      "ì•„ë‹ˆ: 0.0153\n",
      "ê·¸ë˜: 0.0152\n",
      "ê·¸ëŸ¼: 0.0145\n",
      "ì–´ë–»ê²Œ: 0.0139\n",
      "ìš°ë¦¬: 0.0138\n",
      "ë¬´ìŠ¨: 0.0136\n",
      "ì •ë§: 0.0131\n",
      "ì‚´ë ¤ì£¼ì„¸ìš”: 0.0131\n",
      "ë„ˆê°€: 0.0128\n",
      "ì•„ë‹ˆì•¼: 0.0125\n",
      "ì œê°€: 0.0122\n",
      "ë‹¹ì¥: 0.0117\n",
      "ìˆì–´: 0.0115\n",
      "ê·¸ë ‡ê²Œ: 0.0114\n",
      "ë‚˜ë„: 0.0100\n",
      "ë¹¨ë¦¬: 0.0099\n",
      "ë¯¸ì•ˆí•´: 0.0098\n",
      "ì•Œì•„: 0.0093\n",
      "ê·¸ê²Œ: 0.0091\n",
      "ì´ë ‡ê²Œ: 0.0091\n",
      "ì´ì œ: 0.0090\n",
      "ì—†ì–´: 0.0088\n",
      "ì´ê±°: 0.0087\n",
      "ê±°ì•¼: 0.0087\n",
      "ë‚˜í•œí…Œ: 0.0083\n",
      "ê·¸ê±´: 0.0082\n",
      "í•˜ë©´: 0.0079\n",
      "ë‚˜ëŠ”: 0.0079\n",
      "ë‹¹ì‹ : 0.0079\n",
      "ì´ê²Œ: 0.0078\n",
      "ìê¾¸: 0.0077\n",
      "ë„ˆë¬´: 0.0076\n",
      "ì£½ì—¬ë²„ë¦´ê±°ì•¼: 0.0076\n",
      "ì£„ì†¡í•´ìš”: 0.0076\n",
      "ë„ˆë„: 0.0075\n",
      "í•œë²ˆë§Œ: 0.0075\n",
      "ì˜¤ëŠ˜: 0.0074\n",
      "ê°™ì´: 0.0073\n",
      "ë„¤ê°€: 0.0071\n",
      "ì£½ê³ : 0.0070\n",
      "ê·¸ëŸ¬ë©´: 0.0070\n",
      "ì‹œê°„ì„: 0.0069\n",
      "í•˜ë‚˜: 0.0068\n",
      "\n",
      "ğŸ“Œ í´ë˜ìŠ¤: ê°ˆì·¨ ëŒ€í™”\n",
      "ë‚´ê°€: 0.0314\n",
      "ì§„ì§œ: 0.0255\n",
      "ì´ê±°: 0.0231\n",
      "ì•„ë‹ˆ: 0.0211\n",
      "ê·¸ëŸ¼: 0.0208\n",
      "ì§€ê¸ˆ: 0.0198\n",
      "ì—†ì–´ìš”: 0.0189\n",
      "ë‚´ë†”: 0.0188\n",
      "ì•ˆë¼: 0.0187\n",
      "ì—†ì–´: 0.0187\n",
      "ê·¸ë˜: 0.0183\n",
      "ëˆì´: 0.0178\n",
      "ê·¸ëƒ¥: 0.0159\n",
      "ì œë°œ: 0.0150\n",
      "ë¹¨ë¦¬: 0.0140\n",
      "ë‚˜ë„: 0.0136\n",
      "ê·¸ê±´: 0.0120\n",
      "ì œê°€: 0.0119\n",
      "ë¬´ìŠ¨: 0.0119\n",
      "ì˜¤ëŠ˜: 0.0108\n",
      "ì •ë§: 0.0106\n",
      "ì—¬ê¸°: 0.0105\n",
      "ìˆì–´: 0.0102\n",
      "ì–´ë–»ê²Œ: 0.0100\n",
      "ì•ˆë¼ìš”: 0.0099\n",
      "ì•„ë‹ˆì•¼: 0.0098\n",
      "ì´ê²Œ: 0.0096\n",
      "ìš°ë¦¬: 0.0094\n",
      "ê±°ê¸°: 0.0094\n",
      "ë¯¸ì•ˆí•´: 0.0093\n",
      "ê·¸ê±°: 0.0092\n",
      "ì•Œê² ì–´: 0.0092\n",
      "ì´ê±´: 0.0092\n",
      "ê·¼ë°: 0.0091\n",
      "ë‹ˆê°€: 0.0090\n",
      "ì¤„ë˜: 0.0090\n",
      "ë‚˜ì˜¤ë©´: 0.0090\n",
      "ë‚˜í•œí…Œ: 0.0088\n",
      "ë¹Œë ¤ì¤˜: 0.0084\n",
      "ê·¸ê²Œ: 0.0083\n",
      "ì €ìš”: 0.0082\n",
      "ë„ˆë¬´: 0.0082\n",
      "ëˆì„: 0.0081\n",
      "ìˆëŠ”: 0.0081\n",
      "ì—†ëŠ”ë°: 0.0080\n",
      "ë¯¸ì•ˆ: 0.0079\n",
      "ë‹¹ì¥: 0.0078\n",
      "ì–´ì´: 0.0077\n",
      "ì´ì œ: 0.0076\n",
      "ë’¤ì ¸ì„œ: 0.0075\n"
     ]
    }
   ],
   "source": [
    "classes = df['class'].unique()\n",
    "\n",
    "for label in classes:\n",
    "    subset = df[df['class'] == label]['conversation']\n",
    "    tfidf = vectorizer.fit_transform(subset)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    mean_scores = tfidf.mean(axis=0).A1  # í‰ê·  TF-IDF\n",
    "    top_indices = mean_scores.argsort()[::-1][:50]\n",
    "    \n",
    "    print(f\"\\nğŸ“Œ í´ë˜ìŠ¤: {label}\")\n",
    "    for i in top_indices:\n",
    "        print(f\"{feature_names[i]}: {mean_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b794f577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Vocabulary of size 3 doesn't contain index 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136/637179672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conversation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_custom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \"\"\"\n\u001b[1;32m   2068\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m                             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                         )\n\u001b[0;32m--> 485\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty vocabulary passed to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Vocabulary of size 3 doesn't contain index 0."
     ]
    }
   ],
   "source": [
    "# íŠ¹ì • ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "custom_vocab = {'í˜‘ë°•': 3, 'ì£½ì¸ë‹¤': 5, 'ì¡°ì‹¬í•´': 2}\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, vocabulary=custom_vocab)\n",
    "X_custom = vectorizer.fit_transform(df['conversation'])\n",
    "\n",
    "print(X_custom.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824c62d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5821d664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ê°ˆì·¨ ëŒ€í™”  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì¼ë°˜ ëŒ€í™”  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  í˜‘ë°• ëŒ€í™”\n",
      "0         0          0      0            1      0\n",
      "1         0          0      1            0      0\n",
      "2         0          1      0            0      0\n",
      "3         0          0      1            0      0\n",
      "4         0          0      0            1      0\n",
      "...     ...        ...    ...          ...    ...\n",
      "4632      0          0      0            1      0\n",
      "4633      0          0      0            0      1\n",
      "4634      1          0      0            0      0\n",
      "4635      1          0      0            0      0\n",
      "4636      0          0      1            0      0\n",
      "\n",
      "[4637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# class ì»¬ëŸ¼ ì›-í•« ì¸ì½”ë”©\n",
    "df = train_data\n",
    "class_dummies = pd.get_dummies(df['class'])\n",
    "print(class_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0adf6f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      í˜‘ë°• ëŒ€í™”  ê°ˆì·¨ ëŒ€í™”  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì¼ë°˜ ëŒ€í™”\n",
      "0         0      0            1          0      0\n",
      "1         0      0            0          0      1\n",
      "2         0      0            0          1      0\n",
      "3         0      0            0          0      1\n",
      "4         0      0            1          0      0\n",
      "...     ...    ...          ...        ...    ...\n",
      "4632      0      0            1          0      0\n",
      "4633      1      0            0          0      0\n",
      "4634      0      1            0          0      0\n",
      "4635      0      1            0          0      0\n",
      "4636      0      0            0          0      1\n",
      "\n",
      "[4637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "class_order = ['í˜‘ë°• ëŒ€í™”', 'ê°ˆì·¨ ëŒ€í™”', 'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”', 'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”', 'ì¼ë°˜ ëŒ€í™”']\n",
    "class_dummies = class_dummies[class_order]\n",
    "print(class_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371b8762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = class_dummies.to_numpy()\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d4f575",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx        class                                       conversation  \\\n",
      "0  1951  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  íŒ€ì¥ë‹˜ ì´ê±° ì–¸ì œê¹Œì§€ ë§ˆë¬´ë¦¬ í•˜ë©´ ë ê¹Œìš”?\\në¬´ë¦¬í•˜ì§€ ë§ê³  ë„‰ë„‰í•˜ê²Œ ì£¼ë§ê¹Œì§€ ë‹¤ ì‘...   \n",
      "1  4756        ì¼ë°˜ ëŒ€í™”  ë‚´ì¼ ë‚ ì”¨ ì–´ë–»ëŒ€?\\në¹„ ì˜¨ë‹¤ë˜ë°. ìš°ì‚° ì±™ê²¨ê°€ì•¼ í•  ê²ƒ ê°™ì•„.\\nì—ì´, ì•¼ì™¸ í™œë™...   \n",
      "2  1234    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì•¼ ìŸ¤ ì¢€ ë´.\\n ê¼´ì— ìœ í–‰í•˜ëŠ” ì˜· ì…ì—ˆë„¤ \\n í˜¸ë°•ì— ì¤„ ê¸‹ëŠ”ë‹¤ê³  ìˆ˜ë°•ë˜ë‚˜ \\n...   \n",
      "\n",
      "   í˜‘ë°• ëŒ€í™”  ê°ˆì·¨ ëŒ€í™”  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì¼ë°˜ ëŒ€í™”  \n",
      "0      0      0            1          0      0  \n",
      "1      0      0            0          0      1  \n",
      "2      0      0            0          1      0  \n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì¡´ dfì— ì¸ì½”ë”© ê²°ê³¼ ë¶™ì´ê¸°\n",
    "df_with_dummies = pd.concat([df, class_dummies], axis=1)\n",
    "\n",
    "print(df_with_dummies[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beomi/kcbert-baseëŠ” ìš•ì„¤, ìœ„í˜‘, í˜ì˜¤ ë°œì–¸ ë“± ë¹„ì¼ìƒì ì¸ í‘œí˜„ì´ ë§ì€ ë°ì´í„°ì…‹ì— íŠ¹íˆ ì í•©í•©ë‹ˆë‹¤.\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "#tokens = tokenizer(\"ë„ˆ ì •ë§ ì£½ê³  ì‹¶ëƒ?\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í˜•ëŒ€ì†Œ ë¶„ì„ê¸° ê¸°ë°˜ í† í¬ë‚˜ì´ì € \n",
    "# Mecab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
