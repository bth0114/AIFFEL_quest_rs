{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84beb7e9",
   "metadata": {},
   "source": [
    "## 데이타 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a885d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5ee4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1951</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>팀장님 이거 언제까지 마무리 하면 될까요?\\n무리하지 말고 넉넉하게 주말까지 다 작...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4756</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>내일 날씨 어떻대?\\n비 온다던데. 우산 챙겨가야 할 것 같아.\\n에이, 야외 활동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1234</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>야 쟤 좀 봐.\\n 꼴에 유행하는 옷 입었네 \\n 호박에 줄 긋는다고 수박되나 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4767</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 수업 내용 이해했어?\\n솔직히 좀 어려웠어. 너는?\\n나도 몇 부분이 헷갈리더...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1511</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>자네 내일 아침에 우리집 들려서 출근하게\\n네?\\n내가 차가 고장났어\\n아. 그런데...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx        class                                       conversation\n",
       "0  1951  직장 내 괴롭힘 대화  팀장님 이거 언제까지 마무리 하면 될까요?\\n무리하지 말고 넉넉하게 주말까지 다 작...\n",
       "1  4756        일반 대화  내일 날씨 어떻대?\\n비 온다던데. 우산 챙겨가야 할 것 같아.\\n에이, 야외 활동...\n",
       "2  1234    기타 괴롭힘 대화  야 쟤 좀 봐.\\n 꼴에 유행하는 옷 입었네 \\n 호박에 줄 긋는다고 수박되나 \\n...\n",
       "3  4767        일반 대화  오늘 수업 내용 이해했어?\\n솔직히 좀 어려웠어. 너는?\\n나도 몇 부분이 헷갈리더...\n",
       "4  1511  직장 내 괴롭힘 대화  자네 내일 아침에 우리집 들려서 출근하게\\n네?\\n내가 차가 고장났어\\n아. 그런데..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '../../data', 'raw_csv')\n",
    "train_data_path = os.path.join(data_dir, 'merged_train.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e0ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 4637\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4637 entries, 0 to 4636\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   idx           4637 non-null   int64 \n",
      " 1   class         4637 non-null   object\n",
      " 2   conversation  4637 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 108.8+ KB\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플수 :', (len(train_data)))\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1511da3",
   "metadata": {},
   "source": [
    "**한국어 불용어 사전**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54171b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = ['나', '너', '네', '내','어', '고', '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c84dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', '년', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하', '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또', '문제', '더', '사회', '많', '그리고', '좋', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', '안', '어떤', '내', '경우', '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자', '개', '들', '사실', '이렇', '점', '싶', '말', '정도', '좀', '원', '잘', '통하', '소리', '놓']\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/koreanStopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1afdd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\\s]', '', text)  # 한글, 영문, 공백 제외한 문자 제거\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.strip()  # 양 끝 공백 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workpalce = [상사, 부서장, 팀장, 후배, 상급자, 부하직원, 선배, 동료, 부서, 팀, 부서원, 간호사, 간호부서, 간호단위 관리자]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bullying = [소리 지르다, 화내다, 업무 떠넘기다, 업무 제외, 업무 감시, 업무 간섭, 차별 대우, 비협조, 비꼬다, 모욕하다, 욕하다, 안 좋은 소문, 비방하다, 험담하다, 의심하다, 차가운 말투, 의심하는 눈빛, 차별, 불이익, 압박, 강요, 억압, 배제, 따돌림, 무시, 무관심 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0abd3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0131de1",
   "metadata": {},
   "source": [
    "**Mecab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "262fe6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 상위 단어\n",
      "[('하', 9853), ('어', 9167), ('고', 7436), ('야', 7180), ('아', 5893), ('거', 5489), ('있', 5208), ('지', 5161), ('해', 4813), ('안', 4028), ('게', 4003), ('다', 3958), ('을', 3795), ('말', 3658), ('면', 3509), ('겠', 3397), ('아니', 3214), ('없', 2967), ('주', 2661), ('뭐', 2646), ('왜', 2556), ('만', 2501), ('알', 2423), ('좋', 2386), ('니', 2348), ('습니다', 2151), ('돈', 1997), ('님', 1982), ('일', 1934), ('죄송', 1892)]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Mecab : 형태소 분석기\n",
    "#         형태소 추출(morphs), 명사 추출, 형태소와 태그 추출 \n",
    "tokenizer = Mecab()\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in df['conversation']:\n",
    "    clean_sentence = clean_text(sentence)\n",
    "    temp_X = tokenizer.morphs(clean_sentence) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    all_tokens.append(temp_X)\n",
    "\n",
    "words = np.concatenate(all_tokens).tolist()\n",
    "#flattened_tokens = [token for sublist in all_tokens for token in sublist]\n",
    "    \n",
    "all_freqs = Counter(words).most_common(50)\n",
    "print(\"전체 상위 단어\")\n",
    "print(all_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d1e0a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [직장 내 괴롭힘 대화] 상위 단어:\n",
      "[('는', 2166), ('고', 1770), ('네', 1591), ('에', 1276), ('거', 1243), ('도', 1231), ('야', 1181), ('습니다', 1119), ('죄송', 1055), ('아', 1025), ('어', 1008), ('겠', 994), ('해', 992), ('합니다', 983), ('님', 981), ('은', 894), ('게', 775), ('면', 753), ('을', 743), ('다', 739), ('너', 563), ('제', 562), ('저', 554), ('뭐', 539), ('왜', 484), ('대리', 448), ('회사', 447), ('는데', 442), ('했', 441), ('할', 420), ('못', 416), ('시', 398), ('를', 381), ('만', 379), ('요', 365), ('오늘', 345), ('라고', 326), ('로', 315), ('김', 310), ('부장', 307), ('으로', 306), ('니', 303), ('어요', 289), ('무슨', 287), ('서', 283), ('그럼', 275), ('까지', 270), ('기', 253), ('세요', 252), ('너무', 238)]\n",
      "\n",
      " [일반 대화] 상위 단어:\n",
      "[('어', 3192), ('에', 1482), ('는', 1406), ('아', 1264), ('은', 969), ('해', 935), ('고', 863), ('요즘', 821), ('도', 808), ('야', 691), ('을', 648), ('뭐', 624), ('다', 575), ('겠', 497), ('정말', 412), ('게', 403), ('어떻게', 395), ('오늘', 380), ('었', 375), ('이번', 353), ('주말', 351), ('네', 343), ('응', 338), ('먹', 336), ('재밌', 320), ('했', 318), ('영화', 312), ('를', 295), ('할', 292), ('자', 290), ('거', 278), ('어때', 272), ('고마워', 272), ('계획', 271), ('봤', 261), ('됐', 246), ('어디', 235), ('면', 224), ('같이', 218), ('책', 216), ('시', 215), ('로', 214), ('의', 203), ('에서', 201), ('어요', 197), ('읽', 196), ('습니다', 196), ('그거', 194), ('지내', 193), ('맞', 192)]\n",
      "\n",
      " [기타 괴롭힘 대화] 상위 단어:\n",
      "[('야', 1910), ('는', 1904), ('너', 1552), ('고', 1538), ('아', 1453), ('어', 1414), ('거', 1345), ('도', 1180), ('해', 1156), ('네', 1081), ('에', 912), ('다', 905), ('왜', 870), ('게', 831), ('뭐', 679), ('니', 678), ('은', 668), ('면', 636), ('냐', 625), ('을', 601), ('겠', 527), ('님', 482), ('는데', 480), ('진짜', 444), ('라고', 419), ('세요', 371), ('만', 358), ('마', 356), ('못', 344), ('했', 332), ('그만', 318), ('그래', 317), ('한테', 314), ('봐', 311), ('죄송', 306), ('잖아', 303), ('그냥', 296), ('애', 290), ('무슨', 289), ('시', 289), ('합니다', 281), ('를', 276), ('서', 270), ('할', 265), ('너무', 261), ('저', 257), ('기', 241), ('미안', 240), ('맞', 237), ('어요', 231)]\n",
      "\n",
      " [협박 대화] 상위 단어:\n",
      "[('어', 1859), ('는', 1855), ('야', 1777), ('고', 1707), ('너', 1586), ('거', 1483), ('에', 1135), ('을', 1127), ('도', 1097), ('해', 1047), ('게', 1042), ('아', 1011), ('면', 973), ('네', 966), ('다', 863), ('겠', 810), ('니', 804), ('은', 746), ('죽', 711), ('왜', 623), ('만', 618), ('세요', 579), ('했', 528), ('를', 516), ('할', 477), ('제발', 460), ('못', 447), ('로', 423), ('뭐', 416), ('습니다', 395), ('죄송', 355), ('라고', 349), ('진짜', 340), ('는데', 307), ('으면', 299), ('시', 299), ('합니다', 297), ('기', 295), ('한테', 291), ('자', 287), ('으로', 286), ('그래', 284), ('냐', 282), ('무슨', 277), ('까지', 271), ('봐', 268), ('제', 267), ('저', 258), ('서', 255), ('그냥', 250)]\n",
      "\n",
      " [갈취 대화] 상위 단어:\n",
      "[('어', 1694), ('야', 1621), ('고', 1558), ('돈', 1522), ('는', 1405), ('아', 1140), ('거', 1140), ('에', 1129), ('네', 1067), ('만', 1052), ('너', 998), ('도', 955), ('게', 952), ('면', 923), ('다', 876), ('은', 700), ('해', 683), ('을', 676), ('겠', 569), ('왜', 560), ('세요', 537), ('어요', 464), ('니', 463), ('는데', 459), ('진짜', 455), ('이거', 415), ('요', 413), ('냐', 405), ('저', 397), ('그럼', 389), ('뭐', 388), ('시', 375), ('한테', 360), ('돼', 345), ('줘', 340), ('빌려', 338), ('그래', 321), ('로', 321), ('봐', 313), ('사', 308), ('할', 302), ('맞', 283), ('님', 283), ('잖아', 281), ('제', 273), ('내놔', 268), ('라', 257), ('그냥', 251), ('라고', 249), ('못', 249)]\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 빈도 계산\n",
    "class_freqs = {}\n",
    "for label in df['class'].unique():\n",
    "    class_tokens = []\n",
    "    sub_df = df[df['class'] == label]\n",
    "    for conv in sub_df['conversation']:\n",
    "        clean_sentence = clean_text(conv)\n",
    "        tokens = tokenizer.morphs(clean_sentence) # 토큰화\n",
    "        tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "        class_tokens.extend(tokens)\n",
    "    \n",
    "    class_freqs[label] = Counter(class_tokens).most_common(50)\n",
    "\n",
    "for key, value in class_freqs.items():\n",
    "    print(f\"\\n [{key}] 상위 단어:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3b0f1",
   "metadata": {},
   "source": [
    "**Okt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ddc53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 품사 태깅\n",
    "def pos_tagging(text):\n",
    "    # 관형사, 감탄사, 조사, 어미 등은 불용어 대상으로 토큰화 과정에 제외\n",
    "    # 부사는 우리의 task에 중요한 정보라 판단하여 포함(\"진짜 짜증나\", \"너 정말 못됐다\")\n",
    "    return [word for word, pos in okt.pos(text) if pos in ['Noun', 'Verb', 'Adjective', 'Adverb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "790ccada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 상위 단어\n",
      "[('너', 4622), ('내', 3896), ('나', 2720), ('말', 2640), ('네', 2599), ('왜', 2485), ('뭐', 2377), ('거', 2262), ('좀', 2073), ('돈', 1930), ('해', 1752), ('다', 1750), ('있어', 1583), ('것', 1513), ('이', 1481), ('진짜', 1470), ('죄송합니다', 1425), ('저', 1412), ('지금', 1407), ('니', 1402), ('아니', 1364), ('제', 1263), ('그럼', 1252), ('그래', 1215), ('오늘', 1157), ('우리', 1112), ('안', 1109), ('생각', 1094), ('할', 1049), ('요즘', 1043)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 전체 품사별 빈도수 계산\n",
    "all_tokens = []\n",
    "for conv in df['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(30)\n",
    "print(\"전체 상위 단어\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c02e6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "협박 상위 단어\n",
      "[('너', 1506), ('왜', 599), ('니', 570), ('다', 492), ('제발', 461), ('해', 433), ('네', 409), ('거', 392), ('뭐', 342), ('진짜', 338), ('그래', 286), ('제', 252), ('그냥', 248), ('죽여', 248), ('죄송합니다', 240), ('저', 233), ('돈', 228), ('할', 225), ('그럼', 218), ('하면', 217), ('정말', 202), ('어떻게', 201), ('무슨', 199), ('새끼', 195), ('못', 185), ('당신', 172), ('봐', 171), ('가족', 170), ('있어', 165), ('난', 164), ('칼', 164), ('여기', 163), ('넌', 161), ('그렇게', 161), ('신고', 161), ('아니야', 161), ('당장', 157), ('이제', 146), ('살려주세요', 143), ('딸', 141), ('애', 137), ('어디', 133), ('죽', 130), ('빨리', 129), ('한번', 129), ('나도', 127), ('이렇게', 125), ('하지', 122), ('돼', 121), ('죽어', 120), ('전', 120), ('줄', 119), ('없어', 118), ('경찰', 117), ('하는', 116), ('그게', 116), ('뭘', 114), ('날', 113), ('오늘', 112), ('버릴거야', 112), ('게', 101), ('정신', 100), ('눈', 98), ('했어', 97), ('그건', 95), ('걸', 94), ('입', 93), ('미안해', 93), ('같아', 92), ('너무', 91), ('건', 90), ('그러면', 87), ('누구', 86), ('하겠습니다', 86), ('친구', 83), ('자꾸', 82), ('같이', 81), ('협박', 81), ('엄마', 80), ('놈', 79), ('해봐', 79), ('목숨', 78), ('동생', 77), ('아들', 77), ('죽고', 76), ('이번', 75), ('장기', 74), ('잘못', 73), ('같은', 73), ('널', 73), ('손', 72), ('있는', 72), ('조금', 72), ('말로', 71), ('아이', 70), ('죄송해요', 69), ('아주', 67), ('얘기', 67), ('오빠', 67), ('대로', 66)]\n"
     ]
    }
   ],
   "source": [
    "# 협박 대화 빈도수 계산\n",
    "subset = df[ (df['class'] == \"협박 대화\") ]\n",
    "all_tokens = []\n",
    "\n",
    "for conv in subset['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(100)\n",
    "print(\"협박 상위 단어\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76dbee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "괴롭힘 상위 단어\n",
      "[('왜', 1327), ('말', 1285), ('죄송합니다', 1095), ('뭐', 1059), ('거', 984), ('아니', 782), ('해', 751), ('저', 750), ('제', 730), ('다', 712), ('진짜', 629), ('일', 628), ('지금', 618), ('것', 590), ('니', 547), ('우리', 542), ('그래', 538), ('안', 531), ('사람', 485), ('그럼', 479), ('그냥', 470), ('오늘', 462), ('봐', 457), ('대리', 457), ('회사', 456), ('그렇게', 447), ('무슨', 431), ('할', 403), ('너무', 400), ('하는', 364), ('알', 355), ('애', 344), ('생각', 337), ('부장', 317), ('이렇게', 311), ('여기', 307), ('더', 307), ('그', 298), ('못', 293), ('때', 286), ('새끼', 281), ('아니야', 280), ('김', 279), ('이번', 274), ('어떻게', 273), ('집', 270), ('시간', 270), ('그게', 263), ('하면', 261), ('하나', 256), ('말씀', 253), ('요', 246), ('넌', 245), ('아닙니다', 241), ('뭘', 239), ('자네', 237), ('빨리', 237), ('하겠습니다', 237), ('돈', 234), ('그건', 233), ('응', 231), ('하지', 229), ('과장', 228), ('하고', 221), ('팀', 219), ('해서', 219), ('수', 219), ('정말', 215), ('그런', 214), ('친구', 207), ('내일', 204), ('그래도', 204), ('걸', 204), ('다시', 203), ('근데', 202), ('앞', 201), ('업무', 191), ('줄', 187), ('게', 186), ('없어', 184), ('알겠습니다', 183), ('제발', 181), ('얘', 180), ('엄마', 179), ('다른', 176), ('같은', 176), ('있어', 175), ('고객', 175), ('난', 171), ('얼굴', 171), ('휴가', 170), ('같이', 170), ('돼', 168), ('그거', 168), ('기분', 160), ('저기', 159), ('어디', 157), ('손님', 157), ('했어', 155), ('옷', 154)]\n"
     ]
    }
   ],
   "source": [
    "# 괴롭힘 대화 빈도수 계산\n",
    "subset = df[ (df['class'] == \"직장 내 괴롭힘 대화\") | (df['class'] == \"기타 괴롭힘 대화\")]\n",
    "all_tokens = []\n",
    "\n",
    "for conv in subset['conversation']:\n",
    "    tokens = pos_tagging(clean_text(conv))\n",
    "    tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "all_freqs = Counter(all_tokens).most_common(100)\n",
    "print(\"괴롭힘 상위 단어\")\n",
    "print(all_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64b17def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [직장 내 괴롭힘 대화] 상위 단어:\n",
      "[('네', 1095), ('죄송합니다', 871), ('너', 542), ('제', 528), ('저', 509), ('거', 483), ('뭐', 482), ('왜', 478), ('대리', 455), ('회사', 447), ('해', 418), ('다', 413), ('오늘', 344), ('부장', 316), ('그럼', 274), ('김', 272), ('할', 260), ('그래', 233), ('과장', 228), ('자네', 228), ('무슨', 221), ('아닙니다', 215), ('하겠습니다', 212), ('하는', 211), ('팀', 210), ('이번', 210), ('그렇게', 207), ('너무', 195), ('업무', 188), ('봐', 187), ('진짜', 185), ('말씀', 184), ('이렇게', 180), ('그냥', 175), ('하면', 173), ('내일', 170), ('휴가', 168), ('요', 164), ('못', 161), ('알겠습니다', 158), ('그게', 152), ('그건', 145), ('해서', 142), ('하고', 141), ('그래도', 135), ('커피', 135), ('어떻게', 131), ('빨리', 129), ('니', 129), ('정말', 129)]\n",
      "\n",
      " [일반 대화] 상위 단어:\n",
      "[('있어', 1100), ('요즘', 821), ('뭐', 623), ('정말', 412), ('같아', 407), ('어떻게', 396), ('오늘', 380), ('좋은', 378), ('이번', 353), ('주말', 351), ('해', 340), ('응', 338), ('영화', 309), ('나도', 304), ('어때', 273), ('고마워', 272), ('계획', 271), ('좋아', 261), ('했어', 247), ('할', 242), ('됐어', 229), ('같이', 218), ('책', 218), ('어땠어', 200), ('어디', 194), ('그거', 194), ('좋아해', 190), ('추천', 177), ('지내', 175), ('재밌는', 175), ('그럼', 172), ('새로', 169), ('네', 163), ('저녁', 161), ('거', 160), ('날씨', 159), ('다음', 155), ('준비', 153), ('싶어', 147), ('커피', 145), ('산', 143), ('시', 139), ('봤어', 139), ('너', 137), ('그렇게', 136), ('음악', 136), ('친구', 131), ('그래서', 131), ('할래', 130), ('본', 129)]\n",
      "\n",
      " [기타 괴롭힘 대화] 상위 단어:\n",
      "[('너', 1477), ('왜', 849), ('뭐', 577), ('거', 501), ('진짜', 444), ('니', 418), ('네', 395), ('해', 333), ('그래', 305), ('다', 299), ('그냥', 295), ('봐', 270), ('저', 241), ('그렇게', 240), ('애', 230), ('죄송합니다', 224), ('아니야', 212), ('무슨', 210), ('그럼', 205), ('너무', 205), ('제', 202), ('여기', 200), ('응', 195), ('새끼', 192), ('고객', 173), ('엄마', 168), ('넌', 163), ('얘', 160), ('친구', 159), ('돈', 154), ('하는', 153), ('뭘', 149), ('제발', 149), ('손님', 146), ('할', 143), ('어떻게', 142), ('냄새', 139), ('못', 132), ('난', 132), ('이렇게', 131), ('얼굴', 131), ('하지', 129), ('그만해', 128), ('환불', 121), ('오늘', 118), ('하지마', 116), ('기분', 116), ('근데', 112), ('미안해', 112), ('그게', 111)]\n",
      "\n",
      " [협박 대화] 상위 단어:\n",
      "[('너', 1506), ('왜', 599), ('니', 570), ('다', 492), ('제발', 461), ('해', 433), ('네', 409), ('거', 392), ('뭐', 342), ('진짜', 338), ('그래', 286), ('제', 252), ('그냥', 248), ('죽여', 248), ('죄송합니다', 240), ('저', 233), ('돈', 228), ('할', 225), ('그럼', 218), ('하면', 217), ('정말', 202), ('어떻게', 201), ('무슨', 199), ('새끼', 195), ('못', 185), ('당신', 172), ('봐', 171), ('가족', 170), ('있어', 165), ('난', 164), ('칼', 164), ('여기', 163), ('넌', 161), ('그렇게', 161), ('신고', 161), ('아니야', 161), ('당장', 157), ('이제', 146), ('살려주세요', 143), ('딸', 141), ('애', 137), ('어디', 133), ('죽', 130), ('빨리', 129), ('한번', 129), ('나도', 127), ('이렇게', 125), ('하지', 122), ('돼', 121), ('죽어', 120)]\n",
      "\n",
      " [갈취 대화] 상위 단어:\n",
      "[('돈', 1466), ('너', 960), ('거', 726), ('왜', 540), ('네', 537), ('진짜', 458), ('다', 456), ('만원', 401), ('그럼', 383), ('저', 380), ('뭐', 353), ('돼', 334), ('그래', 316), ('봐', 288), ('니', 284), ('없어', 268), ('내놔', 267), ('제', 261), ('그냥', 241), ('제발', 235), ('줄', 228), ('해', 228), ('없어요', 227), ('여기', 206), ('오늘', 203), ('이번', 203), ('빨리', 203), ('엄마', 194), ('줘', 188), ('친구', 187), ('할', 179), ('나도', 173), ('무슨', 169), ('정말', 156), ('그건', 156), ('그거', 150), ('응', 147), ('있어', 143), ('어떻게', 143), ('새끼', 140), ('한번', 121), ('얼마', 119), ('저번', 116), ('돼요', 114), ('빌려', 112), ('근데', 111), ('하면', 111), ('그게', 111), ('아니야', 110), ('내일', 109)]\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 빈도 계산\n",
    "class_freqs = {}\n",
    "for label in df['class'].unique():\n",
    "    class_tokens = []\n",
    "    sub_df = df[df['class'] == label]\n",
    "    for conv in sub_df['conversation']:\n",
    "        tokens = pos_tagging(clean_text(conv))\n",
    "        tokens = [word for word in tokens if not word in stopwords] # 불용어 제거\n",
    "        class_tokens.extend(tokens)\n",
    "        \n",
    "    class_freqs[label] = Counter(class_tokens).most_common(50)\n",
    "\n",
    "for key, value in class_freqs.items():\n",
    "    print(f\"\\n [{key}] 상위 단어:\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a7a57",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "\n",
    "자주 등장하지만 모든 문서에 다 나오는 단어는 가중치를 낮게,\n",
    "특정 문서에서만 자주 등장하는 단어는 가중치를 높게 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b8105b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['conversation'].apply(lambda x: ' '.join(pos_tagging(clean_text(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cd3c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가가' '가감' '가거든요' ... '힘좀' '힙니' '힙합']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df['tokenized'])\n",
    "\n",
    "# 단어 목록\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# TF-IDF 행렬 확인\n",
    "print(X_tfidf.toarray())\n",
    "\n",
    "#print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3abbec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "과장: 0.2091\n",
      "넉넉하게: 0.1926\n",
      "늦어지게: 0.2020\n",
      "됐고: 0.1084\n",
      "될까: 0.1183\n",
      "됩니다: 0.1277\n",
      "마무리: 0.3919\n",
      "무리하지: 0.1926\n",
      "받았어요: 0.1860\n",
      "보내: 0.1199\n",
      "생겼네: 0.1606\n",
      "아뇨: 0.1247\n",
      "아니: 0.0582\n",
      "아직: 0.1823\n",
      "알겠습니다: 0.0874\n",
      "어떡합니까: 0.1766\n",
      "언제: 0.0904\n",
      "없더라구요: 0.2020\n",
      "오늘: 0.1942\n",
      "자리: 0.1114\n",
      "작성: 0.1312\n",
      "장님: 0.2257\n",
      "전달: 0.2821\n",
      "죄송해요: 0.1010\n",
      "주말: 0.1725\n",
      "퇴근: 0.1136\n",
      "하라: 0.1014\n",
      "하면: 0.1430\n",
      "하셔서: 0.1353\n",
      "해달라: 0.1860\n",
      "해서: 0.0782\n",
      "해주시면: 0.1477\n",
      "했나요: 0.1446\n",
      "했는데: 0.1874\n",
      "했어요: 0.1121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0번째 문장의 비어있지 않은 단어만 확인\n",
    "row_0 = X_tfidf[0].toarray().flatten()\n",
    "non_zero_indices = np.where(row_0 > 0)[0]\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in non_zero_indices:\n",
    "    print(f\"{words[i]}: {row_0[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca696780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 클래스: 직장 내 괴롭힘 대화\n",
      "죄송합니다: 0.0404\n",
      "내가: 0.0254\n",
      "아니: 0.0210\n",
      "제가: 0.0197\n",
      "지금: 0.0191\n",
      "그럼: 0.0167\n",
      "부장님: 0.0167\n",
      "오늘: 0.0160\n",
      "아닙니다: 0.0159\n",
      "무슨: 0.0141\n",
      "알겠습니다: 0.0133\n",
      "그래: 0.0130\n",
      "그렇게: 0.0128\n",
      "너무: 0.0125\n",
      "진짜: 0.0120\n",
      "김대리: 0.0118\n",
      "그냥: 0.0117\n",
      "이거: 0.0112\n",
      "회사: 0.0111\n",
      "이렇게: 0.0111\n",
      "과장님: 0.0109\n",
      "그게: 0.0108\n",
      "자네: 0.0106\n",
      "다시: 0.0104\n",
      "그건: 0.0101\n",
      "일이: 0.0101\n",
      "제대로: 0.0099\n",
      "정말: 0.0098\n",
      "우리: 0.0098\n",
      "하고: 0.0098\n",
      "그래도: 0.0097\n",
      "어떻게: 0.0094\n",
      "일을: 0.0092\n",
      "하겠습니다: 0.0090\n",
      "내일: 0.0088\n",
      "빨리: 0.0088\n",
      "다른: 0.0084\n",
      "같습니다: 0.0083\n",
      "요즘: 0.0082\n",
      "아니요: 0.0081\n",
      "저도: 0.0080\n",
      "휴가: 0.0079\n",
      "그런: 0.0078\n",
      "같이: 0.0077\n",
      "이건: 0.0075\n",
      "팀장님: 0.0074\n",
      "없어: 0.0073\n",
      "사람이: 0.0072\n",
      "근데: 0.0072\n",
      "하는: 0.0070\n",
      "\n",
      "📌 클래스: 일반 대화\n",
      "있어: 0.0653\n",
      "요즘: 0.0566\n",
      "정말: 0.0294\n",
      "어떻게: 0.0292\n",
      "같아: 0.0289\n",
      "오늘: 0.0281\n",
      "좋은: 0.0276\n",
      "주말에: 0.0246\n",
      "영화: 0.0246\n",
      "이번: 0.0243\n",
      "어떤: 0.0239\n",
      "나도: 0.0235\n",
      "어때: 0.0230\n",
      "좋아: 0.0225\n",
      "고마워: 0.0216\n",
      "같이: 0.0199\n",
      "됐어: 0.0195\n",
      "어땠어: 0.0185\n",
      "그럼: 0.0183\n",
      "싶어: 0.0183\n",
      "좋아해: 0.0179\n",
      "시간: 0.0179\n",
      "그거: 0.0175\n",
      "지내: 0.0164\n",
      "재밌는: 0.0162\n",
      "나는: 0.0157\n",
      "거야: 0.0155\n",
      "새로: 0.0153\n",
      "계획: 0.0134\n",
      "그렇게: 0.0134\n",
      "생각이야: 0.0133\n",
      "봤어: 0.0133\n",
      "커피: 0.0133\n",
      "그래서: 0.0131\n",
      "저녁: 0.0130\n",
      "맞아: 0.0126\n",
      "생각해: 0.0125\n",
      "있는: 0.0125\n",
      "있나요: 0.0124\n",
      "어제: 0.0122\n",
      "음악: 0.0119\n",
      "어디: 0.0118\n",
      "최근에: 0.0118\n",
      "날씨: 0.0118\n",
      "새로운: 0.0117\n",
      "한잔할래: 0.0115\n",
      "했어: 0.0115\n",
      "너무: 0.0115\n",
      "함께: 0.0111\n",
      "읽고: 0.0107\n",
      "\n",
      "📌 클래스: 기타 괴롭힘 대화\n",
      "내가: 0.0277\n",
      "진짜: 0.0224\n",
      "아니: 0.0197\n",
      "그냥: 0.0174\n",
      "죄송합니다: 0.0160\n",
      "그래: 0.0157\n",
      "아니야: 0.0151\n",
      "니가: 0.0138\n",
      "지금: 0.0137\n",
      "그렇게: 0.0136\n",
      "그럼: 0.0135\n",
      "무슨: 0.0133\n",
      "너무: 0.0130\n",
      "그만해: 0.0120\n",
      "제발: 0.0117\n",
      "우리: 0.0113\n",
      "이거: 0.0111\n",
      "너가: 0.0111\n",
      "미안해: 0.0108\n",
      "고객님: 0.0107\n",
      "여기: 0.0106\n",
      "제가: 0.0099\n",
      "손님: 0.0097\n",
      "어떻게: 0.0096\n",
      "나도: 0.0085\n",
      "그만: 0.0084\n",
      "그게: 0.0083\n",
      "이렇게: 0.0082\n",
      "말이: 0.0082\n",
      "너는: 0.0082\n",
      "근데: 0.0081\n",
      "미안: 0.0081\n",
      "빨리: 0.0081\n",
      "하지마: 0.0080\n",
      "그런: 0.0080\n",
      "나한테: 0.0080\n",
      "뭐가: 0.0078\n",
      "나는: 0.0075\n",
      "뭐야: 0.0073\n",
      "오늘: 0.0072\n",
      "이게: 0.0072\n",
      "싫어: 0.0072\n",
      "그건: 0.0069\n",
      "엄마: 0.0068\n",
      "정말: 0.0068\n",
      "없어: 0.0067\n",
      "있어: 0.0064\n",
      "이제: 0.0063\n",
      "왜그래: 0.0062\n",
      "다시: 0.0060\n",
      "\n",
      "📌 클래스: 협박 대화\n",
      "내가: 0.0372\n",
      "제발: 0.0250\n",
      "진짜: 0.0207\n",
      "죄송합니다: 0.0206\n",
      "지금: 0.0202\n",
      "니가: 0.0165\n",
      "그냥: 0.0162\n",
      "아니: 0.0153\n",
      "그래: 0.0152\n",
      "그럼: 0.0145\n",
      "어떻게: 0.0139\n",
      "우리: 0.0138\n",
      "무슨: 0.0136\n",
      "정말: 0.0131\n",
      "살려주세요: 0.0131\n",
      "너가: 0.0128\n",
      "아니야: 0.0125\n",
      "제가: 0.0122\n",
      "당장: 0.0117\n",
      "있어: 0.0115\n",
      "그렇게: 0.0114\n",
      "나도: 0.0100\n",
      "빨리: 0.0099\n",
      "미안해: 0.0098\n",
      "알아: 0.0093\n",
      "그게: 0.0091\n",
      "이렇게: 0.0091\n",
      "이제: 0.0090\n",
      "없어: 0.0088\n",
      "이거: 0.0087\n",
      "거야: 0.0087\n",
      "나한테: 0.0083\n",
      "그건: 0.0082\n",
      "하면: 0.0079\n",
      "나는: 0.0079\n",
      "당신: 0.0079\n",
      "이게: 0.0078\n",
      "자꾸: 0.0077\n",
      "너무: 0.0076\n",
      "죽여버릴거야: 0.0076\n",
      "죄송해요: 0.0076\n",
      "너도: 0.0075\n",
      "한번만: 0.0075\n",
      "오늘: 0.0074\n",
      "같이: 0.0073\n",
      "네가: 0.0071\n",
      "죽고: 0.0070\n",
      "그러면: 0.0070\n",
      "시간을: 0.0069\n",
      "하나: 0.0068\n",
      "\n",
      "📌 클래스: 갈취 대화\n",
      "내가: 0.0314\n",
      "진짜: 0.0255\n",
      "이거: 0.0231\n",
      "아니: 0.0211\n",
      "그럼: 0.0208\n",
      "지금: 0.0198\n",
      "없어요: 0.0189\n",
      "내놔: 0.0188\n",
      "안돼: 0.0187\n",
      "없어: 0.0187\n",
      "그래: 0.0183\n",
      "돈이: 0.0178\n",
      "그냥: 0.0159\n",
      "제발: 0.0150\n",
      "빨리: 0.0140\n",
      "나도: 0.0136\n",
      "그건: 0.0120\n",
      "제가: 0.0119\n",
      "무슨: 0.0119\n",
      "오늘: 0.0108\n",
      "정말: 0.0106\n",
      "여기: 0.0105\n",
      "있어: 0.0102\n",
      "어떻게: 0.0100\n",
      "안돼요: 0.0099\n",
      "아니야: 0.0098\n",
      "이게: 0.0096\n",
      "우리: 0.0094\n",
      "거기: 0.0094\n",
      "미안해: 0.0093\n",
      "그거: 0.0092\n",
      "알겠어: 0.0092\n",
      "이건: 0.0092\n",
      "근데: 0.0091\n",
      "니가: 0.0090\n",
      "줄래: 0.0090\n",
      "나오면: 0.0090\n",
      "나한테: 0.0088\n",
      "빌려줘: 0.0084\n",
      "그게: 0.0083\n",
      "저요: 0.0082\n",
      "너무: 0.0082\n",
      "돈을: 0.0081\n",
      "있는: 0.0081\n",
      "없는데: 0.0080\n",
      "미안: 0.0079\n",
      "당장: 0.0078\n",
      "어이: 0.0077\n",
      "이제: 0.0076\n",
      "뒤져서: 0.0075\n"
     ]
    }
   ],
   "source": [
    "classes = df['class'].unique()\n",
    "\n",
    "for label in classes:\n",
    "    subset = df[df['class'] == label]['conversation']\n",
    "    tfidf = vectorizer.fit_transform(subset)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    mean_scores = tfidf.mean(axis=0).A1  # 평균 TF-IDF\n",
    "    top_indices = mean_scores.argsort()[::-1][:50]\n",
    "    \n",
    "    print(f\"\\n📌 클래스: {label}\")\n",
    "    for i in top_indices:\n",
    "        print(f\"{feature_names[i]}: {mean_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b794f577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Vocabulary of size 3 doesn't contain index 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136/637179672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conversation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_custom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \"\"\"\n\u001b[1;32m   2068\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0mmin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m                             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                         )\n\u001b[0;32m--> 485\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty vocabulary passed to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Vocabulary of size 3 doesn't contain index 0."
     ]
    }
   ],
   "source": [
    "# 특정 단어에 가중치 추가\n",
    "custom_vocab = {'협박': 3, '죽인다': 5, '조심해': 2}\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, vocabulary=custom_vocab)\n",
    "X_custom = vectorizer.fit_transform(df['conversation'])\n",
    "\n",
    "print(X_custom.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824c62d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**원-핫 인코딩(One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5821d664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      갈취 대화  기타 괴롭힘 대화  일반 대화  직장 내 괴롭힘 대화  협박 대화\n",
      "0         0          0      0            1      0\n",
      "1         0          0      1            0      0\n",
      "2         0          1      0            0      0\n",
      "3         0          0      1            0      0\n",
      "4         0          0      0            1      0\n",
      "...     ...        ...    ...          ...    ...\n",
      "4632      0          0      0            1      0\n",
      "4633      0          0      0            0      1\n",
      "4634      1          0      0            0      0\n",
      "4635      1          0      0            0      0\n",
      "4636      0          0      1            0      0\n",
      "\n",
      "[4637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# class 컬럼 원-핫 인코딩\n",
    "df = train_data\n",
    "class_dummies = pd.get_dummies(df['class'])\n",
    "print(class_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0adf6f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      협박 대화  갈취 대화  직장 내 괴롭힘 대화  기타 괴롭힘 대화  일반 대화\n",
      "0         0      0            1          0      0\n",
      "1         0      0            0          0      1\n",
      "2         0      0            0          1      0\n",
      "3         0      0            0          0      1\n",
      "4         0      0            1          0      0\n",
      "...     ...    ...          ...        ...    ...\n",
      "4632      0      0            1          0      0\n",
      "4633      1      0            0          0      0\n",
      "4634      0      1            0          0      0\n",
      "4635      0      1            0          0      0\n",
      "4636      0      0            0          0      1\n",
      "\n",
      "[4637 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "class_order = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일반 대화']\n",
    "class_dummies = class_dummies[class_order]\n",
    "print(class_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371b8762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = class_dummies.to_numpy()\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d4f575",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idx        class                                       conversation  \\\n",
      "0  1951  직장 내 괴롭힘 대화  팀장님 이거 언제까지 마무리 하면 될까요?\\n무리하지 말고 넉넉하게 주말까지 다 작...   \n",
      "1  4756        일반 대화  내일 날씨 어떻대?\\n비 온다던데. 우산 챙겨가야 할 것 같아.\\n에이, 야외 활동...   \n",
      "2  1234    기타 괴롭힘 대화  야 쟤 좀 봐.\\n 꼴에 유행하는 옷 입었네 \\n 호박에 줄 긋는다고 수박되나 \\n...   \n",
      "\n",
      "   협박 대화  갈취 대화  직장 내 괴롭힘 대화  기타 괴롭힘 대화  일반 대화  \n",
      "0      0      0            1          0      0  \n",
      "1      0      0            0          0      1  \n",
      "2      0      0            0          1      0  \n"
     ]
    }
   ],
   "source": [
    "# 기존 df에 인코딩 결과 붙이기\n",
    "df_with_dummies = pd.concat([df, class_dummies], axis=1)\n",
    "\n",
    "print(df_with_dummies[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beomi/kcbert-base는 욕설, 위협, 혐오 발언 등 비일상적인 표현이 많은 데이터셋에 특히 적합합니다.\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "#tokens = tokenizer(\"너 정말 죽고 싶냐?\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#형대소 분석기 기반 토크나이저 \n",
    "# Mecab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
